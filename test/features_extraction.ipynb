{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet16 features extraction.\n",
    "\n",
    "One of the key components to be able to create a GMVAE of the CXR14 dataset, is to be able to retrieve <br>\n",
    "the VGGne16 features, just as how the paper. <br><br> \"Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest X-ray Images\" has done. <br>\n",
    "link: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6651749/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import necessary modules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "plt.rcParams['figure.figsize'] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to here\n",
    "\n",
    "Make sure the setup the paths properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Python_Projects\\CS236-Final-Proj\\test\n"
     ]
    }
   ],
   "source": [
    "#Path to assign tests (copy path directly)\n",
    "test_path = r\"D:\\Python_Projects\\CS236-Final-Proj\\test\"\n",
    "\n",
    "#Set the path to this working directory\n",
    "os.chdir(test_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "#Append the path the src folder\n",
    "sys.path.append(r'D:\\Python_Projects\\CS236-Final-Proj\\src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary module for downloading\n",
    "\n",
    "Note for this: EVERYTIME There is a change inside the download <br>\n",
    "the changes inside the file would only be shown if the jupyter kernel is restarted. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from utils import CXReader, DfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path\n",
    "data_path = os.path.join(test_path, os.pardir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframes of the data\n",
    "First, lets obtain the dataframes for the data and check that all metadata <br>\n",
    "information has been set up properly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112124/112124 [00:00<00:00, 505359.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_test.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_train.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_val.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe compiler\n",
    "df_compiler = DfReader()\n",
    "\n",
    "#set the path and retrieve the dataframes\n",
    "df_compiler.set_folder_path(data_path)\n",
    "\n",
    "#Get the dataframe holder and names\n",
    "dfs_holder, dfs_names = df_compiler.get_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "It can iterate through all batches\n"
     ]
    }
   ],
   "source": [
    "# Get the device if cuda or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a transformations for the VGGnet16 (requires a 224,224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#Create datasets and dataloaders\n",
    "test_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[0], transform=transform, device=device)\n",
    "train_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[1], transform=transform,device=device)\n",
    "val_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[2], transform=transform, device=device)\n",
    "\n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#With batch size of 32, and shuffle true, and num workers = 4\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print an image and see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8000, 0.7961, 0.7843,  ..., 0.7686, 0.7725, 0.7569],\n",
      "         [0.7961, 0.7882, 0.7725,  ..., 0.7490, 0.7608, 0.7373],\n",
      "         [0.7922, 0.7804, 0.7686,  ..., 0.7333, 0.7451, 0.7216],\n",
      "         ...,\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0392, 0.0118, 0.0000],\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000],\n",
      "         [0.2784, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000]],\n",
      "\n",
      "        [[0.8000, 0.7961, 0.7843,  ..., 0.7686, 0.7725, 0.7569],\n",
      "         [0.7961, 0.7882, 0.7725,  ..., 0.7490, 0.7608, 0.7373],\n",
      "         [0.7922, 0.7804, 0.7686,  ..., 0.7333, 0.7451, 0.7216],\n",
      "         ...,\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0392, 0.0118, 0.0000],\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000],\n",
      "         [0.2784, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000]],\n",
      "\n",
      "        [[0.8000, 0.7961, 0.7843,  ..., 0.7686, 0.7725, 0.7569],\n",
      "         [0.7961, 0.7882, 0.7725,  ..., 0.7490, 0.7608, 0.7373],\n",
      "         [0.7922, 0.7804, 0.7686,  ..., 0.7333, 0.7451, 0.7216],\n",
      "         ...,\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0392, 0.0118, 0.0000],\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000],\n",
      "         [0.2784, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000]]])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(samp3_image)\n",
    "print(samp3_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the vgg16_model features and set to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Projects\\CS236-Final-Proj\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Python_Projects\\CS236-Final-Proj\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1371, 0.3649,  ..., 0.0929, 0.2825, 0.0000],\n",
      "         [0.0000, 0.1291, 0.3101,  ..., 0.2486, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.6302,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1466,  ..., 0.0335, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0788, 0.0000,  ..., 0.0000, 0.0000, 0.1012],\n",
      "         [0.0000, 0.2479, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [1.6533, 1.1819, 0.0000,  ..., 0.8610, 0.6871, 0.3413],\n",
      "         [1.5862, 1.1613, 0.4082,  ..., 1.0885, 1.6131, 0.2470],\n",
      "         [0.3657, 1.2491, 1.5154,  ..., 1.8290, 2.2573, 0.8447]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0399, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [4.5374, 2.9300, 0.0000,  ..., 3.2306, 2.3817, 3.9273],\n",
      "         [2.9298, 3.0299, 0.0000,  ..., 2.6490, 2.3929, 2.5352],\n",
      "         [1.5958, 2.5682, 0.1743,  ..., 0.0000, 0.0000, 0.7477]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "torch.Size([512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "class VGGEncoder(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGGEncoder, self).__init__()\n",
    "\n",
    "        # Load pre-trained VGG16 model\n",
    "        vgg16_model = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "        # Use only the features part and remove the classifier\n",
    "        self.features = vgg16_model.features\n",
    "\n",
    "        # Set to evaluation mode if not fine-tuning\n",
    "        if not pretrained:\n",
    "            self.features.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Example usage\n",
    "# For fine-tuning\n",
    "#encoder_finetune = VGGEncoder(pretrained=False)\n",
    "#encoder_finetune.train()\n",
    "\n",
    "# For inference\n",
    "encoder_inference = VGGEncoder(pretrained=True)\n",
    "encoder_inference.eval()\n",
    "\n",
    "#Print output and output features\n",
    "output_features = encoder_inference(samp3_image)\n",
    "print(output_features)\n",
    "print(output_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have seen the VGG16 layer.\n",
    "Lets implement the solution where we flat the feature vector to a 1x1xD vector. <br>\n",
    "just as how the paper does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 1, 1])\n",
      "tensor([[[[1.9504]],\n",
      "\n",
      "         [[2.8891]],\n",
      "\n",
      "         [[2.0695]],\n",
      "\n",
      "         [[2.5180]],\n",
      "\n",
      "         [[2.1734]],\n",
      "\n",
      "         [[2.2566]],\n",
      "\n",
      "         [[1.7711]],\n",
      "\n",
      "         [[2.1167]],\n",
      "\n",
      "         [[2.3652]],\n",
      "\n",
      "         [[1.6242]],\n",
      "\n",
      "         [[2.0487]],\n",
      "\n",
      "         [[1.9136]],\n",
      "\n",
      "         [[1.9123]],\n",
      "\n",
      "         [[2.0060]],\n",
      "\n",
      "         [[2.7016]],\n",
      "\n",
      "         [[2.7294]],\n",
      "\n",
      "         [[2.6718]],\n",
      "\n",
      "         [[2.2179]],\n",
      "\n",
      "         [[1.9488]],\n",
      "\n",
      "         [[1.9449]],\n",
      "\n",
      "         [[2.2826]],\n",
      "\n",
      "         [[1.8497]],\n",
      "\n",
      "         [[2.1873]],\n",
      "\n",
      "         [[1.3241]],\n",
      "\n",
      "         [[2.9704]],\n",
      "\n",
      "         [[2.4062]],\n",
      "\n",
      "         [[3.1645]],\n",
      "\n",
      "         [[1.9799]],\n",
      "\n",
      "         [[1.5627]],\n",
      "\n",
      "         [[2.3195]],\n",
      "\n",
      "         [[2.0308]],\n",
      "\n",
      "         [[2.5041]],\n",
      "\n",
      "         [[2.2661]],\n",
      "\n",
      "         [[2.4397]],\n",
      "\n",
      "         [[2.0639]],\n",
      "\n",
      "         [[1.9733]],\n",
      "\n",
      "         [[2.4823]],\n",
      "\n",
      "         [[2.3043]],\n",
      "\n",
      "         [[2.3744]],\n",
      "\n",
      "         [[2.2648]],\n",
      "\n",
      "         [[2.5281]],\n",
      "\n",
      "         [[1.8426]],\n",
      "\n",
      "         [[2.3494]],\n",
      "\n",
      "         [[1.9849]],\n",
      "\n",
      "         [[3.2582]],\n",
      "\n",
      "         [[2.4915]],\n",
      "\n",
      "         [[2.3310]],\n",
      "\n",
      "         [[3.3814]],\n",
      "\n",
      "         [[2.4157]],\n",
      "\n",
      "         [[2.4543]],\n",
      "\n",
      "         [[2.3411]],\n",
      "\n",
      "         [[2.5322]],\n",
      "\n",
      "         [[2.1734]],\n",
      "\n",
      "         [[2.1953]],\n",
      "\n",
      "         [[2.3948]],\n",
      "\n",
      "         [[2.1773]],\n",
      "\n",
      "         [[2.6073]],\n",
      "\n",
      "         [[2.3860]],\n",
      "\n",
      "         [[2.9401]],\n",
      "\n",
      "         [[1.9577]],\n",
      "\n",
      "         [[4.0378]],\n",
      "\n",
      "         [[2.8969]],\n",
      "\n",
      "         [[1.9974]],\n",
      "\n",
      "         [[3.4525]],\n",
      "\n",
      "         [[2.4757]],\n",
      "\n",
      "         [[2.2086]],\n",
      "\n",
      "         [[1.9910]],\n",
      "\n",
      "         [[1.7117]],\n",
      "\n",
      "         [[2.2409]],\n",
      "\n",
      "         [[2.0792]],\n",
      "\n",
      "         [[2.1192]],\n",
      "\n",
      "         [[1.8193]],\n",
      "\n",
      "         [[2.6424]],\n",
      "\n",
      "         [[2.2855]],\n",
      "\n",
      "         [[2.0498]],\n",
      "\n",
      "         [[2.5030]],\n",
      "\n",
      "         [[2.0422]],\n",
      "\n",
      "         [[1.6243]],\n",
      "\n",
      "         [[2.0692]],\n",
      "\n",
      "         [[2.3835]],\n",
      "\n",
      "         [[2.5100]],\n",
      "\n",
      "         [[2.0149]],\n",
      "\n",
      "         [[2.6378]],\n",
      "\n",
      "         [[1.7592]],\n",
      "\n",
      "         [[2.2283]],\n",
      "\n",
      "         [[2.1493]],\n",
      "\n",
      "         [[1.8781]],\n",
      "\n",
      "         [[2.0766]],\n",
      "\n",
      "         [[2.0184]],\n",
      "\n",
      "         [[1.7170]],\n",
      "\n",
      "         [[1.8468]],\n",
      "\n",
      "         [[2.4976]],\n",
      "\n",
      "         [[2.4489]],\n",
      "\n",
      "         [[2.1262]],\n",
      "\n",
      "         [[1.7454]],\n",
      "\n",
      "         [[2.4298]],\n",
      "\n",
      "         [[2.8919]],\n",
      "\n",
      "         [[1.6351]],\n",
      "\n",
      "         [[2.2700]],\n",
      "\n",
      "         [[2.1792]],\n",
      "\n",
      "         [[2.1998]],\n",
      "\n",
      "         [[3.1946]],\n",
      "\n",
      "         [[2.2735]],\n",
      "\n",
      "         [[1.5347]],\n",
      "\n",
      "         [[1.8941]],\n",
      "\n",
      "         [[1.7303]],\n",
      "\n",
      "         [[2.1765]],\n",
      "\n",
      "         [[2.7650]],\n",
      "\n",
      "         [[2.7910]],\n",
      "\n",
      "         [[2.3712]],\n",
      "\n",
      "         [[2.6201]],\n",
      "\n",
      "         [[2.7715]],\n",
      "\n",
      "         [[2.6916]],\n",
      "\n",
      "         [[2.4117]],\n",
      "\n",
      "         [[2.3108]],\n",
      "\n",
      "         [[2.2456]],\n",
      "\n",
      "         [[1.8725]],\n",
      "\n",
      "         [[1.7593]],\n",
      "\n",
      "         [[1.5010]],\n",
      "\n",
      "         [[2.6827]],\n",
      "\n",
      "         [[1.7674]],\n",
      "\n",
      "         [[2.7690]],\n",
      "\n",
      "         [[2.5571]],\n",
      "\n",
      "         [[2.6432]],\n",
      "\n",
      "         [[1.7040]],\n",
      "\n",
      "         [[2.3359]],\n",
      "\n",
      "         [[1.5525]],\n",
      "\n",
      "         [[2.3007]],\n",
      "\n",
      "         [[1.8863]],\n",
      "\n",
      "         [[2.1915]],\n",
      "\n",
      "         [[2.0562]],\n",
      "\n",
      "         [[1.9229]],\n",
      "\n",
      "         [[2.8687]],\n",
      "\n",
      "         [[2.1848]],\n",
      "\n",
      "         [[2.0323]],\n",
      "\n",
      "         [[2.0817]],\n",
      "\n",
      "         [[2.0868]],\n",
      "\n",
      "         [[2.5016]],\n",
      "\n",
      "         [[2.5627]],\n",
      "\n",
      "         [[3.1141]],\n",
      "\n",
      "         [[2.1198]],\n",
      "\n",
      "         [[1.4808]],\n",
      "\n",
      "         [[2.8887]],\n",
      "\n",
      "         [[2.3172]],\n",
      "\n",
      "         [[1.9482]],\n",
      "\n",
      "         [[1.9227]],\n",
      "\n",
      "         [[2.3742]],\n",
      "\n",
      "         [[1.3744]],\n",
      "\n",
      "         [[2.2916]],\n",
      "\n",
      "         [[2.1872]],\n",
      "\n",
      "         [[2.0435]],\n",
      "\n",
      "         [[1.6951]],\n",
      "\n",
      "         [[2.5977]],\n",
      "\n",
      "         [[2.2943]],\n",
      "\n",
      "         [[1.5262]],\n",
      "\n",
      "         [[2.6148]],\n",
      "\n",
      "         [[2.1004]],\n",
      "\n",
      "         [[2.5130]],\n",
      "\n",
      "         [[2.0938]],\n",
      "\n",
      "         [[2.1838]],\n",
      "\n",
      "         [[1.6914]],\n",
      "\n",
      "         [[2.9661]],\n",
      "\n",
      "         [[2.1427]],\n",
      "\n",
      "         [[2.3540]],\n",
      "\n",
      "         [[1.5532]],\n",
      "\n",
      "         [[1.9093]],\n",
      "\n",
      "         [[2.1054]],\n",
      "\n",
      "         [[2.4560]],\n",
      "\n",
      "         [[2.0939]],\n",
      "\n",
      "         [[3.4360]],\n",
      "\n",
      "         [[2.6643]],\n",
      "\n",
      "         [[1.9402]],\n",
      "\n",
      "         [[2.3416]],\n",
      "\n",
      "         [[2.5550]],\n",
      "\n",
      "         [[2.2486]],\n",
      "\n",
      "         [[1.6710]],\n",
      "\n",
      "         [[2.2931]],\n",
      "\n",
      "         [[2.1532]],\n",
      "\n",
      "         [[1.5543]],\n",
      "\n",
      "         [[2.2613]],\n",
      "\n",
      "         [[2.6107]],\n",
      "\n",
      "         [[2.2732]],\n",
      "\n",
      "         [[2.1515]],\n",
      "\n",
      "         [[2.0779]],\n",
      "\n",
      "         [[2.4213]],\n",
      "\n",
      "         [[3.1258]],\n",
      "\n",
      "         [[1.6332]],\n",
      "\n",
      "         [[2.1536]],\n",
      "\n",
      "         [[1.8122]],\n",
      "\n",
      "         [[1.6313]],\n",
      "\n",
      "         [[3.4974]],\n",
      "\n",
      "         [[2.6969]],\n",
      "\n",
      "         [[2.5999]],\n",
      "\n",
      "         [[1.8000]],\n",
      "\n",
      "         [[1.8432]],\n",
      "\n",
      "         [[2.7426]],\n",
      "\n",
      "         [[2.8822]],\n",
      "\n",
      "         [[2.5166]],\n",
      "\n",
      "         [[2.6514]],\n",
      "\n",
      "         [[2.3952]],\n",
      "\n",
      "         [[2.6493]],\n",
      "\n",
      "         [[2.2998]],\n",
      "\n",
      "         [[1.8425]],\n",
      "\n",
      "         [[1.6877]],\n",
      "\n",
      "         [[1.8178]],\n",
      "\n",
      "         [[2.3769]],\n",
      "\n",
      "         [[2.2368]],\n",
      "\n",
      "         [[3.3397]],\n",
      "\n",
      "         [[1.9860]],\n",
      "\n",
      "         [[2.0167]],\n",
      "\n",
      "         [[1.9880]],\n",
      "\n",
      "         [[3.8959]],\n",
      "\n",
      "         [[1.7893]],\n",
      "\n",
      "         [[2.0785]],\n",
      "\n",
      "         [[3.4113]],\n",
      "\n",
      "         [[1.9832]],\n",
      "\n",
      "         [[1.9019]],\n",
      "\n",
      "         [[2.0213]],\n",
      "\n",
      "         [[1.7955]],\n",
      "\n",
      "         [[2.7496]],\n",
      "\n",
      "         [[2.6795]],\n",
      "\n",
      "         [[2.0113]],\n",
      "\n",
      "         [[2.0928]],\n",
      "\n",
      "         [[2.0795]],\n",
      "\n",
      "         [[2.2019]],\n",
      "\n",
      "         [[2.2434]],\n",
      "\n",
      "         [[2.1655]],\n",
      "\n",
      "         [[2.2995]],\n",
      "\n",
      "         [[1.9800]],\n",
      "\n",
      "         [[2.6287]],\n",
      "\n",
      "         [[2.5772]],\n",
      "\n",
      "         [[1.7354]],\n",
      "\n",
      "         [[2.2677]],\n",
      "\n",
      "         [[2.8154]],\n",
      "\n",
      "         [[2.0231]],\n",
      "\n",
      "         [[2.7415]],\n",
      "\n",
      "         [[2.5233]],\n",
      "\n",
      "         [[1.9613]],\n",
      "\n",
      "         [[2.1743]],\n",
      "\n",
      "         [[2.1173]],\n",
      "\n",
      "         [[1.9905]],\n",
      "\n",
      "         [[1.9944]],\n",
      "\n",
      "         [[2.2554]],\n",
      "\n",
      "         [[1.7627]],\n",
      "\n",
      "         [[2.6526]],\n",
      "\n",
      "         [[2.3445]],\n",
      "\n",
      "         [[2.5278]],\n",
      "\n",
      "         [[2.3607]],\n",
      "\n",
      "         [[2.3659]],\n",
      "\n",
      "         [[1.7059]],\n",
      "\n",
      "         [[2.0728]],\n",
      "\n",
      "         [[1.9898]],\n",
      "\n",
      "         [[1.8465]],\n",
      "\n",
      "         [[2.3865]],\n",
      "\n",
      "         [[2.3309]],\n",
      "\n",
      "         [[1.7897]],\n",
      "\n",
      "         [[2.4599]],\n",
      "\n",
      "         [[2.9296]],\n",
      "\n",
      "         [[2.5080]],\n",
      "\n",
      "         [[2.6830]],\n",
      "\n",
      "         [[3.0920]],\n",
      "\n",
      "         [[2.0567]],\n",
      "\n",
      "         [[2.4078]],\n",
      "\n",
      "         [[2.0100]],\n",
      "\n",
      "         [[2.0604]],\n",
      "\n",
      "         [[2.5084]],\n",
      "\n",
      "         [[2.3466]],\n",
      "\n",
      "         [[2.1039]],\n",
      "\n",
      "         [[2.4130]],\n",
      "\n",
      "         [[3.8578]],\n",
      "\n",
      "         [[2.0739]],\n",
      "\n",
      "         [[2.8759]],\n",
      "\n",
      "         [[2.8528]],\n",
      "\n",
      "         [[2.2587]],\n",
      "\n",
      "         [[1.8859]],\n",
      "\n",
      "         [[1.9572]],\n",
      "\n",
      "         [[2.2132]],\n",
      "\n",
      "         [[1.9495]],\n",
      "\n",
      "         [[1.7625]],\n",
      "\n",
      "         [[2.8465]],\n",
      "\n",
      "         [[2.4611]],\n",
      "\n",
      "         [[2.3190]],\n",
      "\n",
      "         [[1.5259]],\n",
      "\n",
      "         [[2.8043]],\n",
      "\n",
      "         [[3.1030]],\n",
      "\n",
      "         [[2.6956]],\n",
      "\n",
      "         [[2.5960]],\n",
      "\n",
      "         [[1.7188]],\n",
      "\n",
      "         [[1.8752]],\n",
      "\n",
      "         [[2.5719]],\n",
      "\n",
      "         [[2.3862]],\n",
      "\n",
      "         [[2.3413]],\n",
      "\n",
      "         [[2.1803]],\n",
      "\n",
      "         [[1.9252]],\n",
      "\n",
      "         [[1.8098]],\n",
      "\n",
      "         [[2.4209]],\n",
      "\n",
      "         [[2.5228]],\n",
      "\n",
      "         [[1.8610]],\n",
      "\n",
      "         [[2.4217]],\n",
      "\n",
      "         [[1.9064]]],\n",
      "\n",
      "\n",
      "        [[[1.6147]],\n",
      "\n",
      "         [[1.9188]],\n",
      "\n",
      "         [[1.9166]],\n",
      "\n",
      "         [[2.6758]],\n",
      "\n",
      "         [[2.8200]],\n",
      "\n",
      "         [[2.4915]],\n",
      "\n",
      "         [[2.8625]],\n",
      "\n",
      "         [[1.9484]],\n",
      "\n",
      "         [[2.5507]],\n",
      "\n",
      "         [[1.8191]],\n",
      "\n",
      "         [[3.1841]],\n",
      "\n",
      "         [[2.1661]],\n",
      "\n",
      "         [[2.6187]],\n",
      "\n",
      "         [[2.0939]],\n",
      "\n",
      "         [[2.2586]],\n",
      "\n",
      "         [[1.5483]],\n",
      "\n",
      "         [[2.8288]],\n",
      "\n",
      "         [[3.1061]],\n",
      "\n",
      "         [[2.0410]],\n",
      "\n",
      "         [[1.6456]],\n",
      "\n",
      "         [[1.6185]],\n",
      "\n",
      "         [[2.5713]],\n",
      "\n",
      "         [[2.9653]],\n",
      "\n",
      "         [[2.6570]],\n",
      "\n",
      "         [[2.5962]],\n",
      "\n",
      "         [[2.2476]],\n",
      "\n",
      "         [[2.3924]],\n",
      "\n",
      "         [[2.1255]],\n",
      "\n",
      "         [[3.7710]],\n",
      "\n",
      "         [[2.6627]],\n",
      "\n",
      "         [[2.4755]],\n",
      "\n",
      "         [[2.3551]],\n",
      "\n",
      "         [[2.8750]],\n",
      "\n",
      "         [[2.0228]],\n",
      "\n",
      "         [[2.1179]],\n",
      "\n",
      "         [[1.9500]],\n",
      "\n",
      "         [[1.6345]],\n",
      "\n",
      "         [[1.9837]],\n",
      "\n",
      "         [[2.6757]],\n",
      "\n",
      "         [[2.5770]],\n",
      "\n",
      "         [[2.0856]],\n",
      "\n",
      "         [[2.5424]],\n",
      "\n",
      "         [[1.8582]],\n",
      "\n",
      "         [[1.9252]],\n",
      "\n",
      "         [[2.0536]],\n",
      "\n",
      "         [[2.0978]],\n",
      "\n",
      "         [[1.7722]],\n",
      "\n",
      "         [[2.2465]],\n",
      "\n",
      "         [[1.4626]],\n",
      "\n",
      "         [[2.1961]],\n",
      "\n",
      "         [[1.4709]],\n",
      "\n",
      "         [[1.9554]],\n",
      "\n",
      "         [[3.4252]],\n",
      "\n",
      "         [[2.0120]],\n",
      "\n",
      "         [[1.8518]],\n",
      "\n",
      "         [[2.1941]],\n",
      "\n",
      "         [[2.9642]],\n",
      "\n",
      "         [[3.0468]],\n",
      "\n",
      "         [[2.2753]],\n",
      "\n",
      "         [[2.3340]],\n",
      "\n",
      "         [[3.3777]],\n",
      "\n",
      "         [[1.7934]],\n",
      "\n",
      "         [[2.2516]],\n",
      "\n",
      "         [[2.1301]],\n",
      "\n",
      "         [[2.2998]],\n",
      "\n",
      "         [[2.4203]],\n",
      "\n",
      "         [[1.9802]],\n",
      "\n",
      "         [[2.6604]],\n",
      "\n",
      "         [[2.3673]],\n",
      "\n",
      "         [[2.4682]],\n",
      "\n",
      "         [[1.9153]],\n",
      "\n",
      "         [[2.3610]],\n",
      "\n",
      "         [[2.1559]],\n",
      "\n",
      "         [[1.4204]],\n",
      "\n",
      "         [[2.8606]],\n",
      "\n",
      "         [[1.7113]],\n",
      "\n",
      "         [[2.6847]],\n",
      "\n",
      "         [[1.8191]],\n",
      "\n",
      "         [[2.2564]],\n",
      "\n",
      "         [[1.9217]],\n",
      "\n",
      "         [[1.9429]],\n",
      "\n",
      "         [[1.9914]],\n",
      "\n",
      "         [[1.8432]],\n",
      "\n",
      "         [[2.2823]],\n",
      "\n",
      "         [[2.1810]],\n",
      "\n",
      "         [[2.9549]],\n",
      "\n",
      "         [[2.2320]],\n",
      "\n",
      "         [[2.1329]],\n",
      "\n",
      "         [[1.5431]],\n",
      "\n",
      "         [[2.1427]],\n",
      "\n",
      "         [[2.1661]],\n",
      "\n",
      "         [[2.6474]],\n",
      "\n",
      "         [[1.9630]],\n",
      "\n",
      "         [[2.1770]],\n",
      "\n",
      "         [[3.0374]],\n",
      "\n",
      "         [[1.6025]],\n",
      "\n",
      "         [[2.0392]],\n",
      "\n",
      "         [[3.2684]],\n",
      "\n",
      "         [[2.5302]],\n",
      "\n",
      "         [[2.6036]],\n",
      "\n",
      "         [[2.4634]],\n",
      "\n",
      "         [[1.6604]],\n",
      "\n",
      "         [[1.8794]],\n",
      "\n",
      "         [[2.6671]],\n",
      "\n",
      "         [[2.4267]],\n",
      "\n",
      "         [[3.9406]],\n",
      "\n",
      "         [[2.5579]],\n",
      "\n",
      "         [[2.1913]],\n",
      "\n",
      "         [[1.8802]],\n",
      "\n",
      "         [[1.9501]],\n",
      "\n",
      "         [[1.9197]],\n",
      "\n",
      "         [[2.7616]],\n",
      "\n",
      "         [[1.8763]],\n",
      "\n",
      "         [[1.3025]],\n",
      "\n",
      "         [[3.4089]],\n",
      "\n",
      "         [[2.0265]],\n",
      "\n",
      "         [[3.1178]],\n",
      "\n",
      "         [[3.3068]],\n",
      "\n",
      "         [[2.7422]],\n",
      "\n",
      "         [[2.2078]],\n",
      "\n",
      "         [[2.4662]],\n",
      "\n",
      "         [[2.3184]],\n",
      "\n",
      "         [[2.0774]],\n",
      "\n",
      "         [[2.5426]],\n",
      "\n",
      "         [[2.0164]],\n",
      "\n",
      "         [[1.6287]],\n",
      "\n",
      "         [[2.7454]],\n",
      "\n",
      "         [[2.1668]],\n",
      "\n",
      "         [[2.0826]],\n",
      "\n",
      "         [[2.3001]],\n",
      "\n",
      "         [[2.8378]],\n",
      "\n",
      "         [[2.0795]],\n",
      "\n",
      "         [[1.3799]],\n",
      "\n",
      "         [[2.1771]],\n",
      "\n",
      "         [[1.6988]],\n",
      "\n",
      "         [[1.8004]],\n",
      "\n",
      "         [[2.4265]],\n",
      "\n",
      "         [[2.0920]],\n",
      "\n",
      "         [[2.6547]],\n",
      "\n",
      "         [[2.3099]],\n",
      "\n",
      "         [[3.0133]],\n",
      "\n",
      "         [[2.5619]],\n",
      "\n",
      "         [[1.7664]],\n",
      "\n",
      "         [[2.9528]],\n",
      "\n",
      "         [[2.3326]],\n",
      "\n",
      "         [[2.4264]],\n",
      "\n",
      "         [[2.4917]],\n",
      "\n",
      "         [[2.9709]],\n",
      "\n",
      "         [[2.3689]],\n",
      "\n",
      "         [[2.7496]],\n",
      "\n",
      "         [[2.2643]],\n",
      "\n",
      "         [[1.6244]],\n",
      "\n",
      "         [[1.7006]],\n",
      "\n",
      "         [[1.9501]],\n",
      "\n",
      "         [[2.7176]],\n",
      "\n",
      "         [[2.2273]],\n",
      "\n",
      "         [[2.5442]],\n",
      "\n",
      "         [[1.9718]],\n",
      "\n",
      "         [[1.6322]],\n",
      "\n",
      "         [[2.2731]],\n",
      "\n",
      "         [[2.4841]],\n",
      "\n",
      "         [[1.2221]],\n",
      "\n",
      "         [[2.0022]],\n",
      "\n",
      "         [[1.9765]],\n",
      "\n",
      "         [[1.5807]],\n",
      "\n",
      "         [[2.1270]],\n",
      "\n",
      "         [[1.9484]],\n",
      "\n",
      "         [[2.2476]],\n",
      "\n",
      "         [[2.4889]],\n",
      "\n",
      "         [[1.6116]],\n",
      "\n",
      "         [[2.6933]],\n",
      "\n",
      "         [[2.1101]],\n",
      "\n",
      "         [[1.8400]],\n",
      "\n",
      "         [[2.7716]],\n",
      "\n",
      "         [[1.3346]],\n",
      "\n",
      "         [[1.9214]],\n",
      "\n",
      "         [[2.1921]],\n",
      "\n",
      "         [[2.2365]],\n",
      "\n",
      "         [[2.8701]],\n",
      "\n",
      "         [[2.5677]],\n",
      "\n",
      "         [[1.5342]],\n",
      "\n",
      "         [[1.7917]],\n",
      "\n",
      "         [[2.4507]],\n",
      "\n",
      "         [[3.1696]],\n",
      "\n",
      "         [[2.1672]],\n",
      "\n",
      "         [[2.8407]],\n",
      "\n",
      "         [[2.5713]],\n",
      "\n",
      "         [[2.0350]],\n",
      "\n",
      "         [[2.2457]],\n",
      "\n",
      "         [[2.2122]],\n",
      "\n",
      "         [[1.8451]],\n",
      "\n",
      "         [[1.8718]],\n",
      "\n",
      "         [[1.8688]],\n",
      "\n",
      "         [[2.4368]],\n",
      "\n",
      "         [[1.8900]],\n",
      "\n",
      "         [[2.3477]],\n",
      "\n",
      "         [[1.5068]],\n",
      "\n",
      "         [[2.8732]],\n",
      "\n",
      "         [[2.4358]],\n",
      "\n",
      "         [[2.9274]],\n",
      "\n",
      "         [[2.1620]],\n",
      "\n",
      "         [[2.4425]],\n",
      "\n",
      "         [[1.7244]],\n",
      "\n",
      "         [[2.1429]],\n",
      "\n",
      "         [[2.1676]],\n",
      "\n",
      "         [[2.7043]],\n",
      "\n",
      "         [[2.9437]],\n",
      "\n",
      "         [[2.2915]],\n",
      "\n",
      "         [[1.7550]],\n",
      "\n",
      "         [[2.4864]],\n",
      "\n",
      "         [[2.0657]],\n",
      "\n",
      "         [[1.7771]],\n",
      "\n",
      "         [[2.5213]],\n",
      "\n",
      "         [[1.9871]],\n",
      "\n",
      "         [[1.2070]],\n",
      "\n",
      "         [[1.6055]],\n",
      "\n",
      "         [[2.3876]],\n",
      "\n",
      "         [[2.5086]],\n",
      "\n",
      "         [[2.2461]],\n",
      "\n",
      "         [[1.5497]],\n",
      "\n",
      "         [[1.5840]],\n",
      "\n",
      "         [[2.4872]],\n",
      "\n",
      "         [[1.8656]],\n",
      "\n",
      "         [[3.6245]],\n",
      "\n",
      "         [[1.7564]],\n",
      "\n",
      "         [[3.2797]],\n",
      "\n",
      "         [[2.6613]],\n",
      "\n",
      "         [[1.8438]],\n",
      "\n",
      "         [[2.7713]],\n",
      "\n",
      "         [[2.5920]],\n",
      "\n",
      "         [[2.0203]],\n",
      "\n",
      "         [[1.5285]],\n",
      "\n",
      "         [[2.5620]],\n",
      "\n",
      "         [[2.7543]],\n",
      "\n",
      "         [[2.0903]],\n",
      "\n",
      "         [[2.9397]],\n",
      "\n",
      "         [[2.0772]],\n",
      "\n",
      "         [[2.8464]],\n",
      "\n",
      "         [[2.2644]],\n",
      "\n",
      "         [[2.4221]],\n",
      "\n",
      "         [[1.9173]],\n",
      "\n",
      "         [[2.0354]],\n",
      "\n",
      "         [[1.7894]],\n",
      "\n",
      "         [[2.2919]],\n",
      "\n",
      "         [[2.7071]],\n",
      "\n",
      "         [[1.9005]],\n",
      "\n",
      "         [[2.9575]],\n",
      "\n",
      "         [[2.0921]],\n",
      "\n",
      "         [[2.7037]],\n",
      "\n",
      "         [[2.2979]],\n",
      "\n",
      "         [[2.7020]],\n",
      "\n",
      "         [[2.0961]],\n",
      "\n",
      "         [[2.0283]],\n",
      "\n",
      "         [[2.1560]],\n",
      "\n",
      "         [[2.0960]],\n",
      "\n",
      "         [[1.5655]],\n",
      "\n",
      "         [[1.8908]],\n",
      "\n",
      "         [[1.7637]],\n",
      "\n",
      "         [[1.6907]],\n",
      "\n",
      "         [[2.1663]],\n",
      "\n",
      "         [[3.8487]],\n",
      "\n",
      "         [[1.8005]],\n",
      "\n",
      "         [[2.1516]],\n",
      "\n",
      "         [[2.6190]],\n",
      "\n",
      "         [[1.9787]],\n",
      "\n",
      "         [[1.7564]],\n",
      "\n",
      "         [[1.6105]],\n",
      "\n",
      "         [[1.9988]],\n",
      "\n",
      "         [[1.9893]],\n",
      "\n",
      "         [[2.1266]],\n",
      "\n",
      "         [[2.8967]],\n",
      "\n",
      "         [[2.1398]],\n",
      "\n",
      "         [[1.7853]],\n",
      "\n",
      "         [[1.7993]],\n",
      "\n",
      "         [[2.1680]],\n",
      "\n",
      "         [[2.8743]],\n",
      "\n",
      "         [[2.8440]],\n",
      "\n",
      "         [[1.9922]],\n",
      "\n",
      "         [[3.2011]],\n",
      "\n",
      "         [[3.1012]],\n",
      "\n",
      "         [[1.9694]],\n",
      "\n",
      "         [[2.2626]],\n",
      "\n",
      "         [[1.6647]],\n",
      "\n",
      "         [[2.0339]],\n",
      "\n",
      "         [[1.6050]],\n",
      "\n",
      "         [[2.0484]],\n",
      "\n",
      "         [[1.6812]],\n",
      "\n",
      "         [[1.7612]],\n",
      "\n",
      "         [[2.0397]],\n",
      "\n",
      "         [[1.9587]],\n",
      "\n",
      "         [[1.7549]],\n",
      "\n",
      "         [[1.8073]],\n",
      "\n",
      "         [[2.5585]],\n",
      "\n",
      "         [[1.7114]],\n",
      "\n",
      "         [[1.9519]],\n",
      "\n",
      "         [[2.0245]],\n",
      "\n",
      "         [[2.4189]],\n",
      "\n",
      "         [[1.8642]],\n",
      "\n",
      "         [[1.9760]],\n",
      "\n",
      "         [[2.2406]]],\n",
      "\n",
      "\n",
      "        [[[2.7791]],\n",
      "\n",
      "         [[2.3225]],\n",
      "\n",
      "         [[2.0333]],\n",
      "\n",
      "         [[3.1546]],\n",
      "\n",
      "         [[1.8709]],\n",
      "\n",
      "         [[2.8995]],\n",
      "\n",
      "         [[2.2147]],\n",
      "\n",
      "         [[3.0334]],\n",
      "\n",
      "         [[2.1187]],\n",
      "\n",
      "         [[2.7927]],\n",
      "\n",
      "         [[2.0861]],\n",
      "\n",
      "         [[1.6459]],\n",
      "\n",
      "         [[1.8204]],\n",
      "\n",
      "         [[1.8424]],\n",
      "\n",
      "         [[2.4456]],\n",
      "\n",
      "         [[1.9032]],\n",
      "\n",
      "         [[2.1248]],\n",
      "\n",
      "         [[3.4300]],\n",
      "\n",
      "         [[3.0873]],\n",
      "\n",
      "         [[2.5703]],\n",
      "\n",
      "         [[1.8450]],\n",
      "\n",
      "         [[2.7398]],\n",
      "\n",
      "         [[2.0964]],\n",
      "\n",
      "         [[2.7212]],\n",
      "\n",
      "         [[2.4407]],\n",
      "\n",
      "         [[1.5104]],\n",
      "\n",
      "         [[2.0157]],\n",
      "\n",
      "         [[3.2342]],\n",
      "\n",
      "         [[2.7797]],\n",
      "\n",
      "         [[2.1015]],\n",
      "\n",
      "         [[1.9219]],\n",
      "\n",
      "         [[1.7324]],\n",
      "\n",
      "         [[2.2069]],\n",
      "\n",
      "         [[2.5050]],\n",
      "\n",
      "         [[1.7768]],\n",
      "\n",
      "         [[1.8702]],\n",
      "\n",
      "         [[2.1973]],\n",
      "\n",
      "         [[2.3157]],\n",
      "\n",
      "         [[2.4008]],\n",
      "\n",
      "         [[2.3754]],\n",
      "\n",
      "         [[3.4618]],\n",
      "\n",
      "         [[1.8870]],\n",
      "\n",
      "         [[2.1053]],\n",
      "\n",
      "         [[2.4639]],\n",
      "\n",
      "         [[1.5223]],\n",
      "\n",
      "         [[2.4098]],\n",
      "\n",
      "         [[3.1539]],\n",
      "\n",
      "         [[1.8339]],\n",
      "\n",
      "         [[1.6696]],\n",
      "\n",
      "         [[2.1678]],\n",
      "\n",
      "         [[1.8547]],\n",
      "\n",
      "         [[2.9280]],\n",
      "\n",
      "         [[1.7971]],\n",
      "\n",
      "         [[1.9620]],\n",
      "\n",
      "         [[1.5520]],\n",
      "\n",
      "         [[2.7191]],\n",
      "\n",
      "         [[2.0323]],\n",
      "\n",
      "         [[3.1090]],\n",
      "\n",
      "         [[1.8999]],\n",
      "\n",
      "         [[2.2844]],\n",
      "\n",
      "         [[2.1731]],\n",
      "\n",
      "         [[3.1965]],\n",
      "\n",
      "         [[2.1418]],\n",
      "\n",
      "         [[2.3281]],\n",
      "\n",
      "         [[2.2428]],\n",
      "\n",
      "         [[2.1355]],\n",
      "\n",
      "         [[1.9432]],\n",
      "\n",
      "         [[1.8196]],\n",
      "\n",
      "         [[1.3050]],\n",
      "\n",
      "         [[2.4489]],\n",
      "\n",
      "         [[2.3693]],\n",
      "\n",
      "         [[2.4232]],\n",
      "\n",
      "         [[1.8861]],\n",
      "\n",
      "         [[2.0444]],\n",
      "\n",
      "         [[2.6050]],\n",
      "\n",
      "         [[2.6268]],\n",
      "\n",
      "         [[2.4319]],\n",
      "\n",
      "         [[2.9023]],\n",
      "\n",
      "         [[2.0128]],\n",
      "\n",
      "         [[1.8094]],\n",
      "\n",
      "         [[2.0442]],\n",
      "\n",
      "         [[2.7081]],\n",
      "\n",
      "         [[2.2163]],\n",
      "\n",
      "         [[2.1710]],\n",
      "\n",
      "         [[2.9243]],\n",
      "\n",
      "         [[1.8567]],\n",
      "\n",
      "         [[2.4778]],\n",
      "\n",
      "         [[1.9399]],\n",
      "\n",
      "         [[2.3770]],\n",
      "\n",
      "         [[1.8003]],\n",
      "\n",
      "         [[1.7490]],\n",
      "\n",
      "         [[2.3311]],\n",
      "\n",
      "         [[1.7912]],\n",
      "\n",
      "         [[2.0204]],\n",
      "\n",
      "         [[3.0003]],\n",
      "\n",
      "         [[1.9734]],\n",
      "\n",
      "         [[3.0704]],\n",
      "\n",
      "         [[2.5465]],\n",
      "\n",
      "         [[1.9045]],\n",
      "\n",
      "         [[1.8770]],\n",
      "\n",
      "         [[2.6230]],\n",
      "\n",
      "         [[1.4349]],\n",
      "\n",
      "         [[2.6984]],\n",
      "\n",
      "         [[2.3462]],\n",
      "\n",
      "         [[2.8581]],\n",
      "\n",
      "         [[2.9660]],\n",
      "\n",
      "         [[2.3626]],\n",
      "\n",
      "         [[1.4307]],\n",
      "\n",
      "         [[1.9274]],\n",
      "\n",
      "         [[2.3765]],\n",
      "\n",
      "         [[1.6681]],\n",
      "\n",
      "         [[1.7890]],\n",
      "\n",
      "         [[3.1901]],\n",
      "\n",
      "         [[2.2114]],\n",
      "\n",
      "         [[1.8606]],\n",
      "\n",
      "         [[2.2662]],\n",
      "\n",
      "         [[2.0406]],\n",
      "\n",
      "         [[1.7503]],\n",
      "\n",
      "         [[2.1504]],\n",
      "\n",
      "         [[1.6389]],\n",
      "\n",
      "         [[1.9401]],\n",
      "\n",
      "         [[2.2392]],\n",
      "\n",
      "         [[2.2107]],\n",
      "\n",
      "         [[1.8828]],\n",
      "\n",
      "         [[2.4626]],\n",
      "\n",
      "         [[3.1415]],\n",
      "\n",
      "         [[2.7951]],\n",
      "\n",
      "         [[1.9858]],\n",
      "\n",
      "         [[2.8426]],\n",
      "\n",
      "         [[2.3848]],\n",
      "\n",
      "         [[2.1272]],\n",
      "\n",
      "         [[2.1718]],\n",
      "\n",
      "         [[2.0611]],\n",
      "\n",
      "         [[2.2654]],\n",
      "\n",
      "         [[3.1189]],\n",
      "\n",
      "         [[2.4767]],\n",
      "\n",
      "         [[1.9156]],\n",
      "\n",
      "         [[2.1731]],\n",
      "\n",
      "         [[1.9612]],\n",
      "\n",
      "         [[1.9528]],\n",
      "\n",
      "         [[2.2493]],\n",
      "\n",
      "         [[2.2578]],\n",
      "\n",
      "         [[1.9390]],\n",
      "\n",
      "         [[2.4188]],\n",
      "\n",
      "         [[2.0030]],\n",
      "\n",
      "         [[2.4141]],\n",
      "\n",
      "         [[1.6367]],\n",
      "\n",
      "         [[2.1424]],\n",
      "\n",
      "         [[2.8344]],\n",
      "\n",
      "         [[1.5982]],\n",
      "\n",
      "         [[1.7369]],\n",
      "\n",
      "         [[2.5627]],\n",
      "\n",
      "         [[2.0614]],\n",
      "\n",
      "         [[1.8520]],\n",
      "\n",
      "         [[1.8559]],\n",
      "\n",
      "         [[2.2880]],\n",
      "\n",
      "         [[2.4062]],\n",
      "\n",
      "         [[2.5073]],\n",
      "\n",
      "         [[2.5610]],\n",
      "\n",
      "         [[1.9219]],\n",
      "\n",
      "         [[2.3634]],\n",
      "\n",
      "         [[2.1173]],\n",
      "\n",
      "         [[2.1260]],\n",
      "\n",
      "         [[2.3002]],\n",
      "\n",
      "         [[2.0844]],\n",
      "\n",
      "         [[1.8954]],\n",
      "\n",
      "         [[1.9917]],\n",
      "\n",
      "         [[2.9699]],\n",
      "\n",
      "         [[2.2106]],\n",
      "\n",
      "         [[2.5832]],\n",
      "\n",
      "         [[1.7002]],\n",
      "\n",
      "         [[2.1614]],\n",
      "\n",
      "         [[2.5387]],\n",
      "\n",
      "         [[1.8513]],\n",
      "\n",
      "         [[2.2763]],\n",
      "\n",
      "         [[1.8434]],\n",
      "\n",
      "         [[1.6770]],\n",
      "\n",
      "         [[3.0513]],\n",
      "\n",
      "         [[3.0904]],\n",
      "\n",
      "         [[1.9416]],\n",
      "\n",
      "         [[2.4702]],\n",
      "\n",
      "         [[2.7582]],\n",
      "\n",
      "         [[1.8095]],\n",
      "\n",
      "         [[2.3697]],\n",
      "\n",
      "         [[2.2642]],\n",
      "\n",
      "         [[1.7779]],\n",
      "\n",
      "         [[3.0142]],\n",
      "\n",
      "         [[1.8006]],\n",
      "\n",
      "         [[2.8601]],\n",
      "\n",
      "         [[1.8311]],\n",
      "\n",
      "         [[2.2180]],\n",
      "\n",
      "         [[2.4504]],\n",
      "\n",
      "         [[1.7877]],\n",
      "\n",
      "         [[2.8960]],\n",
      "\n",
      "         [[2.3502]],\n",
      "\n",
      "         [[2.3716]],\n",
      "\n",
      "         [[2.1141]],\n",
      "\n",
      "         [[2.5589]],\n",
      "\n",
      "         [[1.9284]],\n",
      "\n",
      "         [[1.7037]],\n",
      "\n",
      "         [[1.9873]],\n",
      "\n",
      "         [[1.9967]],\n",
      "\n",
      "         [[2.0994]],\n",
      "\n",
      "         [[2.0686]],\n",
      "\n",
      "         [[1.8965]],\n",
      "\n",
      "         [[2.0574]],\n",
      "\n",
      "         [[3.6711]],\n",
      "\n",
      "         [[1.5797]],\n",
      "\n",
      "         [[2.0283]],\n",
      "\n",
      "         [[2.2610]],\n",
      "\n",
      "         [[2.4796]],\n",
      "\n",
      "         [[2.9971]],\n",
      "\n",
      "         [[2.3679]],\n",
      "\n",
      "         [[2.3934]],\n",
      "\n",
      "         [[2.1326]],\n",
      "\n",
      "         [[2.4808]],\n",
      "\n",
      "         [[2.8922]],\n",
      "\n",
      "         [[1.5872]],\n",
      "\n",
      "         [[1.9186]],\n",
      "\n",
      "         [[2.4089]],\n",
      "\n",
      "         [[2.5143]],\n",
      "\n",
      "         [[2.6682]],\n",
      "\n",
      "         [[2.6431]],\n",
      "\n",
      "         [[2.4876]],\n",
      "\n",
      "         [[2.2356]],\n",
      "\n",
      "         [[1.4295]],\n",
      "\n",
      "         [[2.4755]],\n",
      "\n",
      "         [[3.0587]],\n",
      "\n",
      "         [[2.0388]],\n",
      "\n",
      "         [[1.6532]],\n",
      "\n",
      "         [[1.8486]],\n",
      "\n",
      "         [[2.1221]],\n",
      "\n",
      "         [[2.3215]],\n",
      "\n",
      "         [[1.6969]],\n",
      "\n",
      "         [[1.7411]],\n",
      "\n",
      "         [[1.6114]],\n",
      "\n",
      "         [[2.0504]],\n",
      "\n",
      "         [[2.0786]],\n",
      "\n",
      "         [[2.2258]],\n",
      "\n",
      "         [[2.0338]],\n",
      "\n",
      "         [[2.7386]],\n",
      "\n",
      "         [[1.9921]],\n",
      "\n",
      "         [[1.6849]],\n",
      "\n",
      "         [[2.1531]],\n",
      "\n",
      "         [[1.8349]],\n",
      "\n",
      "         [[2.8159]],\n",
      "\n",
      "         [[1.9180]],\n",
      "\n",
      "         [[3.1439]],\n",
      "\n",
      "         [[2.2921]],\n",
      "\n",
      "         [[1.9436]],\n",
      "\n",
      "         [[2.0364]],\n",
      "\n",
      "         [[2.4680]],\n",
      "\n",
      "         [[2.6580]],\n",
      "\n",
      "         [[2.1772]],\n",
      "\n",
      "         [[2.2976]],\n",
      "\n",
      "         [[2.4809]],\n",
      "\n",
      "         [[2.2535]],\n",
      "\n",
      "         [[2.0186]],\n",
      "\n",
      "         [[2.0651]],\n",
      "\n",
      "         [[1.5707]],\n",
      "\n",
      "         [[2.0597]],\n",
      "\n",
      "         [[2.0651]],\n",
      "\n",
      "         [[2.4058]],\n",
      "\n",
      "         [[2.4763]],\n",
      "\n",
      "         [[2.6696]],\n",
      "\n",
      "         [[2.5796]],\n",
      "\n",
      "         [[2.2157]],\n",
      "\n",
      "         [[2.9268]],\n",
      "\n",
      "         [[2.6185]],\n",
      "\n",
      "         [[2.5636]],\n",
      "\n",
      "         [[1.8827]],\n",
      "\n",
      "         [[2.1975]],\n",
      "\n",
      "         [[2.1848]],\n",
      "\n",
      "         [[1.9800]],\n",
      "\n",
      "         [[1.7217]],\n",
      "\n",
      "         [[2.9764]],\n",
      "\n",
      "         [[2.1993]],\n",
      "\n",
      "         [[2.4314]],\n",
      "\n",
      "         [[3.0506]],\n",
      "\n",
      "         [[2.5314]],\n",
      "\n",
      "         [[2.0466]],\n",
      "\n",
      "         [[2.1469]],\n",
      "\n",
      "         [[2.6604]],\n",
      "\n",
      "         [[2.3735]],\n",
      "\n",
      "         [[1.8549]],\n",
      "\n",
      "         [[3.6828]],\n",
      "\n",
      "         [[2.3319]],\n",
      "\n",
      "         [[2.0195]],\n",
      "\n",
      "         [[1.8344]],\n",
      "\n",
      "         [[1.8200]],\n",
      "\n",
      "         [[2.3476]],\n",
      "\n",
      "         [[2.1235]],\n",
      "\n",
      "         [[3.4885]],\n",
      "\n",
      "         [[2.3576]],\n",
      "\n",
      "         [[3.1392]],\n",
      "\n",
      "         [[1.6360]],\n",
      "\n",
      "         [[2.8189]],\n",
      "\n",
      "         [[3.2574]],\n",
      "\n",
      "         [[2.2317]],\n",
      "\n",
      "         [[2.1926]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        \n",
    "        # Convolutional layer with kernel size 1x1\n",
    "        self.conv1x1 = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Max pooling with kernel size equal to the feature map size\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply operations sequentially\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_channels = 512\n",
    "output_channels = 300 # D is the desired number of channels\n",
    "transition_layer = TransitionLayer(input_channels, output_channels)\n",
    "\n",
    "# Assuming input_tensor is of size (batch_size, 512, 7, 7)\n",
    "input_tensor = torch.randn(3, input_channels, 7, 7)\n",
    "\n",
    "# Apply the transition layer\n",
    "output = transition_layer(input_tensor)\n",
    "\n",
    "# Print the shape of the final feature vector\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIIIIG. We know the feature space at the end would be 512, 7, 7. \n",
    "This means that, if we want to create an encoder that would take this; and convert it to a sampling gaussian representation <br>\n",
    "we need to do the following:\n",
    "1. Pass a VGGnet16 pretrained features at eval mode (option to pretrain it too) to 512, 7, 7.\n",
    "2. Pass that VGGnet16 through a transition layer that would flat it to a num_channelsx 300 output (3 since RGB).\n",
    "3. Use this to sample a mean and gaussian distribution uing homework codes to retrieve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that would sample gaussian parameters.\n",
    "Use functions from hw2 utils.py to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def gaussian_parameters(h, dim=-1):\n",
    "    \"\"\"\n",
    "    Converts generic real-valued representations into mean and variance\n",
    "    parameters of a Gaussian distribution\n",
    "\n",
    "    Args:\n",
    "        h: tensor: (batch, ..., dim, ...): Arbitrary tensor\n",
    "        dim: int: (): Dimension along which to split the tensor for mean and\n",
    "            variance\n",
    "\n",
    "    Returns:\n",
    "        m: tensor: (batch, ..., dim / 2, ...): Mean\n",
    "        v: tensor: (batch, ..., dim / 2, ...): Variance\n",
    "    \"\"\"\n",
    "    m, h = torch.split(h, h.size(dim) // 2, dim=dim)\n",
    "    v = F.softplus(h) + 1e-8\n",
    "    return m, v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
