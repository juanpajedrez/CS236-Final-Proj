{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet16 features extraction.\n",
    "\n",
    "One of the key components to be able to create a GMVAE of the CXR14 dataset, is to be able to retrieve <br>\n",
    "the VGGne16 features, just as how the paper. <br><br> \"Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest X-ray Images\" has done. <br>\n",
    "link: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6651749/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import necessary modules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "plt.rcParams['figure.figsize'] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to here\n",
    "\n",
    "Make sure the setup the paths properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Python_Projects\\CS236-Final-Proj\\test\n"
     ]
    }
   ],
   "source": [
    "#Path to assign tests (copy path directly)\n",
    "test_path = r\"D:\\Python_Projects\\CS236-Final-Proj\\test\"\n",
    "\n",
    "#Set the path to this working directory\n",
    "os.chdir(test_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "#Append the path the src folder\n",
    "sys.path.append(r'D:\\Python_Projects\\CS236-Final-Proj\\src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary module for downloading\n",
    "\n",
    "Note for this: EVERYTIME There is a change inside the download <br>\n",
    "the changes inside the file would only be shown if the jupyter kernel is restarted. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from utils import CXReader, DfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path\n",
    "data_path = os.path.join(test_path, os.pardir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframes of the data\n",
    "First, lets obtain the dataframes for the data and check that all metadata <br>\n",
    "information has been set up properly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/112124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112124/112124 [00:00<00:00, 524524.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_test.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_train.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_val.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe compiler\n",
    "df_compiler = DfReader()\n",
    "\n",
    "#set the path and retrieve the dataframes\n",
    "df_compiler.set_folder_path(data_path)\n",
    "\n",
    "#Get the dataframe holder and names\n",
    "dfs_holder, dfs_names = df_compiler.get_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "It can iterate through all batches\n"
     ]
    }
   ],
   "source": [
    "# Get the device if cuda or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a transformations for the VGGnet16 (requires a 224,224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#Create datasets and dataloaders\n",
    "test_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[0], transform=transform, device=device)\n",
    "train_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[1], transform=transform,device=device)\n",
    "val_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[2], transform=transform, device=device)\n",
    "\n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#With batch size of 16, and shuffle true, and num workers = 4\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print an image and see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8000, 0.7961, 0.7843,  ..., 0.7686, 0.7725, 0.7569],\n",
      "         [0.7961, 0.7882, 0.7725,  ..., 0.7490, 0.7608, 0.7373],\n",
      "         [0.7922, 0.7804, 0.7686,  ..., 0.7333, 0.7451, 0.7216],\n",
      "         ...,\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0392, 0.0118, 0.0000],\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000],\n",
      "         [0.2784, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000]],\n",
      "\n",
      "        [[0.8000, 0.7961, 0.7843,  ..., 0.7686, 0.7725, 0.7569],\n",
      "         [0.7961, 0.7882, 0.7725,  ..., 0.7490, 0.7608, 0.7373],\n",
      "         [0.7922, 0.7804, 0.7686,  ..., 0.7333, 0.7451, 0.7216],\n",
      "         ...,\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0392, 0.0118, 0.0000],\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000],\n",
      "         [0.2784, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000]],\n",
      "\n",
      "        [[0.8000, 0.7961, 0.7843,  ..., 0.7686, 0.7725, 0.7569],\n",
      "         [0.7961, 0.7882, 0.7725,  ..., 0.7490, 0.7608, 0.7373],\n",
      "         [0.7922, 0.7804, 0.7686,  ..., 0.7333, 0.7451, 0.7216],\n",
      "         ...,\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0392, 0.0118, 0.0000],\n",
      "         [0.2745, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000],\n",
      "         [0.2784, 0.1255, 0.0549,  ..., 0.0353, 0.0078, 0.0000]]])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(samp3_image)\n",
    "print(samp3_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the vgg16_model features and set to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Projects\\CS236-Final-Proj\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Python_Projects\\CS236-Final-Proj\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1371, 0.3649,  ..., 0.0929, 0.2825, 0.0000],\n",
      "         [0.0000, 0.1291, 0.3101,  ..., 0.2486, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.6302,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1466,  ..., 0.0335, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0788, 0.0000,  ..., 0.0000, 0.0000, 0.1012],\n",
      "         [0.0000, 0.2479, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [1.6533, 1.1819, 0.0000,  ..., 0.8610, 0.6871, 0.3413],\n",
      "         [1.5862, 1.1613, 0.4082,  ..., 1.0885, 1.6131, 0.2470],\n",
      "         [0.3657, 1.2491, 1.5154,  ..., 1.8290, 2.2573, 0.8447]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0399, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [4.5374, 2.9300, 0.0000,  ..., 3.2306, 2.3817, 3.9273],\n",
      "         [2.9298, 3.0299, 0.0000,  ..., 2.6490, 2.3929, 2.5352],\n",
      "         [1.5958, 2.5682, 0.1743,  ..., 0.0000, 0.0000, 0.7477]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "torch.Size([512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "class VGGEncoder(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGGEncoder, self).__init__()\n",
    "\n",
    "        # Load pre-trained VGG16 model\n",
    "        vgg16_model = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "        # Use only the features part and remove the classifier\n",
    "        self.features = vgg16_model.features\n",
    "\n",
    "        # Set to evaluation mode if not fine-tuning\n",
    "        if not pretrained:\n",
    "            self.features.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Example usage\n",
    "# For fine-tuning\n",
    "#encoder_finetune = VGGEncoder(pretrained=False)\n",
    "#encoder_finetune.train()\n",
    "\n",
    "# For inference\n",
    "encoder_inference = VGGEncoder(pretrained=True)\n",
    "encoder_inference.eval()\n",
    "\n",
    "#Print output and output features\n",
    "output_features = encoder_inference(samp3_image)\n",
    "print(output_features)\n",
    "print(output_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have seen the VGG16 layer.\n",
    "Lets implement the solution where we flat the feature vector to a 1x1xD vector. <br>\n",
    "just as how the paper does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 1, 1])\n",
      "tensor([[[[3.0579]],\n",
      "\n",
      "         [[1.7879]],\n",
      "\n",
      "         [[2.1095]],\n",
      "\n",
      "         [[2.1985]],\n",
      "\n",
      "         [[2.0681]],\n",
      "\n",
      "         [[2.0306]],\n",
      "\n",
      "         [[2.5692]],\n",
      "\n",
      "         [[2.2555]],\n",
      "\n",
      "         [[2.1820]],\n",
      "\n",
      "         [[1.9125]],\n",
      "\n",
      "         [[3.0760]],\n",
      "\n",
      "         [[1.9485]],\n",
      "\n",
      "         [[2.3806]],\n",
      "\n",
      "         [[3.1762]],\n",
      "\n",
      "         [[2.9413]],\n",
      "\n",
      "         [[2.4204]],\n",
      "\n",
      "         [[1.7466]],\n",
      "\n",
      "         [[2.1346]],\n",
      "\n",
      "         [[2.9450]],\n",
      "\n",
      "         [[2.5219]],\n",
      "\n",
      "         [[1.7372]],\n",
      "\n",
      "         [[1.9311]],\n",
      "\n",
      "         [[2.2098]],\n",
      "\n",
      "         [[2.1287]],\n",
      "\n",
      "         [[2.2396]],\n",
      "\n",
      "         [[2.4239]],\n",
      "\n",
      "         [[2.0724]],\n",
      "\n",
      "         [[2.4720]],\n",
      "\n",
      "         [[2.3738]],\n",
      "\n",
      "         [[2.0836]],\n",
      "\n",
      "         [[3.1526]],\n",
      "\n",
      "         [[1.8083]],\n",
      "\n",
      "         [[2.0790]],\n",
      "\n",
      "         [[2.5191]],\n",
      "\n",
      "         [[1.9334]],\n",
      "\n",
      "         [[1.7204]],\n",
      "\n",
      "         [[2.5947]],\n",
      "\n",
      "         [[2.1917]],\n",
      "\n",
      "         [[2.1669]],\n",
      "\n",
      "         [[2.8529]],\n",
      "\n",
      "         [[2.0048]],\n",
      "\n",
      "         [[2.4622]],\n",
      "\n",
      "         [[2.4857]],\n",
      "\n",
      "         [[1.6923]],\n",
      "\n",
      "         [[2.2447]],\n",
      "\n",
      "         [[2.5556]],\n",
      "\n",
      "         [[2.3227]],\n",
      "\n",
      "         [[2.0697]],\n",
      "\n",
      "         [[2.1084]],\n",
      "\n",
      "         [[3.0120]],\n",
      "\n",
      "         [[2.8544]],\n",
      "\n",
      "         [[2.2086]],\n",
      "\n",
      "         [[2.0823]],\n",
      "\n",
      "         [[2.5452]],\n",
      "\n",
      "         [[2.3992]],\n",
      "\n",
      "         [[4.3245]],\n",
      "\n",
      "         [[2.2527]],\n",
      "\n",
      "         [[1.6628]],\n",
      "\n",
      "         [[2.2655]],\n",
      "\n",
      "         [[1.7807]],\n",
      "\n",
      "         [[2.0881]],\n",
      "\n",
      "         [[3.3377]],\n",
      "\n",
      "         [[2.0850]],\n",
      "\n",
      "         [[2.4055]],\n",
      "\n",
      "         [[2.1934]],\n",
      "\n",
      "         [[2.1332]],\n",
      "\n",
      "         [[1.8146]],\n",
      "\n",
      "         [[2.8291]],\n",
      "\n",
      "         [[2.9164]],\n",
      "\n",
      "         [[1.8528]],\n",
      "\n",
      "         [[2.9576]],\n",
      "\n",
      "         [[2.2749]],\n",
      "\n",
      "         [[2.3418]],\n",
      "\n",
      "         [[1.4781]],\n",
      "\n",
      "         [[2.2779]],\n",
      "\n",
      "         [[1.9211]],\n",
      "\n",
      "         [[1.5767]],\n",
      "\n",
      "         [[2.3140]],\n",
      "\n",
      "         [[1.4773]],\n",
      "\n",
      "         [[2.0539]],\n",
      "\n",
      "         [[1.5399]],\n",
      "\n",
      "         [[2.1735]],\n",
      "\n",
      "         [[2.1153]],\n",
      "\n",
      "         [[3.0430]],\n",
      "\n",
      "         [[2.3793]],\n",
      "\n",
      "         [[2.1012]],\n",
      "\n",
      "         [[1.9427]],\n",
      "\n",
      "         [[2.5735]],\n",
      "\n",
      "         [[2.1004]],\n",
      "\n",
      "         [[1.7241]],\n",
      "\n",
      "         [[1.2436]],\n",
      "\n",
      "         [[2.7836]],\n",
      "\n",
      "         [[2.2534]],\n",
      "\n",
      "         [[2.9994]],\n",
      "\n",
      "         [[1.9193]],\n",
      "\n",
      "         [[2.5194]],\n",
      "\n",
      "         [[2.6929]],\n",
      "\n",
      "         [[2.1402]],\n",
      "\n",
      "         [[2.0414]],\n",
      "\n",
      "         [[2.0757]],\n",
      "\n",
      "         [[2.0693]],\n",
      "\n",
      "         [[2.4469]],\n",
      "\n",
      "         [[2.1458]],\n",
      "\n",
      "         [[2.1966]],\n",
      "\n",
      "         [[2.4453]],\n",
      "\n",
      "         [[2.5551]],\n",
      "\n",
      "         [[2.1774]],\n",
      "\n",
      "         [[1.6884]],\n",
      "\n",
      "         [[2.8547]],\n",
      "\n",
      "         [[3.3097]],\n",
      "\n",
      "         [[3.0454]],\n",
      "\n",
      "         [[2.2027]],\n",
      "\n",
      "         [[2.0368]],\n",
      "\n",
      "         [[2.4099]],\n",
      "\n",
      "         [[2.0250]],\n",
      "\n",
      "         [[1.8862]],\n",
      "\n",
      "         [[1.7819]],\n",
      "\n",
      "         [[2.6367]],\n",
      "\n",
      "         [[2.8919]],\n",
      "\n",
      "         [[2.4345]],\n",
      "\n",
      "         [[1.8782]],\n",
      "\n",
      "         [[2.5894]],\n",
      "\n",
      "         [[2.4022]],\n",
      "\n",
      "         [[2.1048]],\n",
      "\n",
      "         [[2.1426]],\n",
      "\n",
      "         [[2.3629]],\n",
      "\n",
      "         [[2.3339]],\n",
      "\n",
      "         [[2.0471]],\n",
      "\n",
      "         [[2.0934]],\n",
      "\n",
      "         [[2.3893]],\n",
      "\n",
      "         [[1.8916]],\n",
      "\n",
      "         [[1.9324]],\n",
      "\n",
      "         [[2.0810]],\n",
      "\n",
      "         [[1.8155]],\n",
      "\n",
      "         [[2.4712]],\n",
      "\n",
      "         [[2.3662]],\n",
      "\n",
      "         [[1.9053]],\n",
      "\n",
      "         [[2.1767]],\n",
      "\n",
      "         [[1.6194]],\n",
      "\n",
      "         [[2.0317]],\n",
      "\n",
      "         [[2.0132]],\n",
      "\n",
      "         [[2.7518]],\n",
      "\n",
      "         [[2.6999]],\n",
      "\n",
      "         [[1.9627]],\n",
      "\n",
      "         [[2.3199]],\n",
      "\n",
      "         [[1.9423]],\n",
      "\n",
      "         [[2.4064]],\n",
      "\n",
      "         [[2.4273]],\n",
      "\n",
      "         [[2.2929]],\n",
      "\n",
      "         [[2.1423]],\n",
      "\n",
      "         [[1.7393]],\n",
      "\n",
      "         [[3.4083]],\n",
      "\n",
      "         [[1.8085]],\n",
      "\n",
      "         [[2.2995]],\n",
      "\n",
      "         [[2.2064]],\n",
      "\n",
      "         [[2.1961]],\n",
      "\n",
      "         [[2.4952]],\n",
      "\n",
      "         [[2.2608]],\n",
      "\n",
      "         [[1.9992]],\n",
      "\n",
      "         [[2.5981]],\n",
      "\n",
      "         [[1.6816]],\n",
      "\n",
      "         [[1.5067]],\n",
      "\n",
      "         [[1.8512]],\n",
      "\n",
      "         [[2.5338]],\n",
      "\n",
      "         [[2.6621]],\n",
      "\n",
      "         [[1.8304]],\n",
      "\n",
      "         [[1.9630]],\n",
      "\n",
      "         [[2.2454]],\n",
      "\n",
      "         [[2.4466]],\n",
      "\n",
      "         [[1.8756]],\n",
      "\n",
      "         [[2.6347]],\n",
      "\n",
      "         [[2.3709]],\n",
      "\n",
      "         [[2.2809]],\n",
      "\n",
      "         [[1.7536]],\n",
      "\n",
      "         [[2.6440]],\n",
      "\n",
      "         [[2.3815]],\n",
      "\n",
      "         [[1.7484]],\n",
      "\n",
      "         [[2.0016]],\n",
      "\n",
      "         [[1.7032]],\n",
      "\n",
      "         [[1.6720]],\n",
      "\n",
      "         [[1.3858]],\n",
      "\n",
      "         [[1.6991]],\n",
      "\n",
      "         [[1.7848]],\n",
      "\n",
      "         [[2.5559]],\n",
      "\n",
      "         [[1.8476]],\n",
      "\n",
      "         [[2.7914]],\n",
      "\n",
      "         [[1.7125]],\n",
      "\n",
      "         [[2.5381]],\n",
      "\n",
      "         [[2.1324]],\n",
      "\n",
      "         [[1.8832]],\n",
      "\n",
      "         [[2.2950]],\n",
      "\n",
      "         [[2.1427]],\n",
      "\n",
      "         [[1.7179]],\n",
      "\n",
      "         [[1.6689]],\n",
      "\n",
      "         [[1.5955]],\n",
      "\n",
      "         [[2.6912]],\n",
      "\n",
      "         [[2.7685]],\n",
      "\n",
      "         [[2.1337]],\n",
      "\n",
      "         [[2.2266]],\n",
      "\n",
      "         [[1.8622]],\n",
      "\n",
      "         [[3.6265]],\n",
      "\n",
      "         [[2.7058]],\n",
      "\n",
      "         [[2.5134]],\n",
      "\n",
      "         [[2.4101]],\n",
      "\n",
      "         [[1.6882]],\n",
      "\n",
      "         [[2.5164]],\n",
      "\n",
      "         [[1.9463]],\n",
      "\n",
      "         [[2.0271]],\n",
      "\n",
      "         [[2.3619]],\n",
      "\n",
      "         [[2.1058]],\n",
      "\n",
      "         [[2.0712]],\n",
      "\n",
      "         [[3.0860]],\n",
      "\n",
      "         [[1.7176]],\n",
      "\n",
      "         [[2.7702]],\n",
      "\n",
      "         [[1.3869]],\n",
      "\n",
      "         [[2.4786]],\n",
      "\n",
      "         [[2.0289]],\n",
      "\n",
      "         [[2.2318]],\n",
      "\n",
      "         [[2.0171]],\n",
      "\n",
      "         [[2.2790]],\n",
      "\n",
      "         [[2.5769]],\n",
      "\n",
      "         [[2.3405]],\n",
      "\n",
      "         [[2.0533]],\n",
      "\n",
      "         [[2.2986]],\n",
      "\n",
      "         [[2.0115]],\n",
      "\n",
      "         [[2.4409]],\n",
      "\n",
      "         [[2.2756]],\n",
      "\n",
      "         [[2.5508]],\n",
      "\n",
      "         [[2.6584]],\n",
      "\n",
      "         [[1.5700]],\n",
      "\n",
      "         [[1.7590]],\n",
      "\n",
      "         [[1.9227]],\n",
      "\n",
      "         [[2.4234]],\n",
      "\n",
      "         [[2.6066]],\n",
      "\n",
      "         [[2.4722]],\n",
      "\n",
      "         [[2.3280]],\n",
      "\n",
      "         [[2.3018]],\n",
      "\n",
      "         [[1.8263]],\n",
      "\n",
      "         [[2.2687]],\n",
      "\n",
      "         [[1.7203]],\n",
      "\n",
      "         [[1.6294]],\n",
      "\n",
      "         [[1.8596]],\n",
      "\n",
      "         [[2.4570]],\n",
      "\n",
      "         [[2.8090]],\n",
      "\n",
      "         [[1.7511]],\n",
      "\n",
      "         [[1.8088]],\n",
      "\n",
      "         [[1.8432]],\n",
      "\n",
      "         [[1.6334]],\n",
      "\n",
      "         [[1.7399]],\n",
      "\n",
      "         [[2.5886]],\n",
      "\n",
      "         [[2.1621]],\n",
      "\n",
      "         [[2.6401]],\n",
      "\n",
      "         [[2.1642]],\n",
      "\n",
      "         [[4.1057]],\n",
      "\n",
      "         [[2.3506]],\n",
      "\n",
      "         [[2.2403]],\n",
      "\n",
      "         [[2.1847]],\n",
      "\n",
      "         [[2.3985]],\n",
      "\n",
      "         [[2.4000]],\n",
      "\n",
      "         [[1.7426]],\n",
      "\n",
      "         [[2.1604]],\n",
      "\n",
      "         [[2.1867]],\n",
      "\n",
      "         [[2.1623]],\n",
      "\n",
      "         [[2.5601]],\n",
      "\n",
      "         [[1.9847]],\n",
      "\n",
      "         [[2.4085]],\n",
      "\n",
      "         [[2.9206]],\n",
      "\n",
      "         [[2.5219]],\n",
      "\n",
      "         [[2.8834]],\n",
      "\n",
      "         [[2.1925]],\n",
      "\n",
      "         [[2.1543]],\n",
      "\n",
      "         [[2.4516]],\n",
      "\n",
      "         [[2.2349]],\n",
      "\n",
      "         [[2.1057]],\n",
      "\n",
      "         [[2.6306]],\n",
      "\n",
      "         [[2.5057]],\n",
      "\n",
      "         [[1.9613]],\n",
      "\n",
      "         [[1.6935]],\n",
      "\n",
      "         [[1.7564]],\n",
      "\n",
      "         [[2.0637]],\n",
      "\n",
      "         [[2.0886]],\n",
      "\n",
      "         [[2.4326]],\n",
      "\n",
      "         [[3.1501]],\n",
      "\n",
      "         [[2.3007]],\n",
      "\n",
      "         [[2.6942]],\n",
      "\n",
      "         [[2.2441]],\n",
      "\n",
      "         [[2.1492]],\n",
      "\n",
      "         [[2.3723]],\n",
      "\n",
      "         [[2.2772]],\n",
      "\n",
      "         [[2.5363]],\n",
      "\n",
      "         [[1.4896]],\n",
      "\n",
      "         [[2.9671]],\n",
      "\n",
      "         [[2.4542]],\n",
      "\n",
      "         [[2.2435]],\n",
      "\n",
      "         [[2.0545]],\n",
      "\n",
      "         [[2.2283]],\n",
      "\n",
      "         [[3.0367]],\n",
      "\n",
      "         [[1.7458]],\n",
      "\n",
      "         [[2.3937]],\n",
      "\n",
      "         [[1.8501]]],\n",
      "\n",
      "\n",
      "        [[[2.7346]],\n",
      "\n",
      "         [[1.9759]],\n",
      "\n",
      "         [[2.6417]],\n",
      "\n",
      "         [[2.6374]],\n",
      "\n",
      "         [[2.5079]],\n",
      "\n",
      "         [[2.8609]],\n",
      "\n",
      "         [[1.7017]],\n",
      "\n",
      "         [[2.9968]],\n",
      "\n",
      "         [[1.7370]],\n",
      "\n",
      "         [[1.8089]],\n",
      "\n",
      "         [[1.6196]],\n",
      "\n",
      "         [[1.8600]],\n",
      "\n",
      "         [[2.1420]],\n",
      "\n",
      "         [[3.0852]],\n",
      "\n",
      "         [[1.7988]],\n",
      "\n",
      "         [[2.3264]],\n",
      "\n",
      "         [[3.5742]],\n",
      "\n",
      "         [[1.8906]],\n",
      "\n",
      "         [[2.4545]],\n",
      "\n",
      "         [[2.0711]],\n",
      "\n",
      "         [[1.2443]],\n",
      "\n",
      "         [[1.9189]],\n",
      "\n",
      "         [[1.8896]],\n",
      "\n",
      "         [[2.0400]],\n",
      "\n",
      "         [[2.7382]],\n",
      "\n",
      "         [[1.7333]],\n",
      "\n",
      "         [[2.6226]],\n",
      "\n",
      "         [[2.1972]],\n",
      "\n",
      "         [[2.7125]],\n",
      "\n",
      "         [[1.9654]],\n",
      "\n",
      "         [[2.4077]],\n",
      "\n",
      "         [[2.1236]],\n",
      "\n",
      "         [[1.9979]],\n",
      "\n",
      "         [[1.5887]],\n",
      "\n",
      "         [[1.7009]],\n",
      "\n",
      "         [[2.9228]],\n",
      "\n",
      "         [[2.4331]],\n",
      "\n",
      "         [[2.1427]],\n",
      "\n",
      "         [[2.5908]],\n",
      "\n",
      "         [[2.0280]],\n",
      "\n",
      "         [[2.9341]],\n",
      "\n",
      "         [[2.4153]],\n",
      "\n",
      "         [[2.4749]],\n",
      "\n",
      "         [[2.4198]],\n",
      "\n",
      "         [[2.0366]],\n",
      "\n",
      "         [[2.3952]],\n",
      "\n",
      "         [[1.9948]],\n",
      "\n",
      "         [[2.5268]],\n",
      "\n",
      "         [[2.0103]],\n",
      "\n",
      "         [[2.0592]],\n",
      "\n",
      "         [[2.2914]],\n",
      "\n",
      "         [[3.5795]],\n",
      "\n",
      "         [[2.4227]],\n",
      "\n",
      "         [[2.0751]],\n",
      "\n",
      "         [[3.4250]],\n",
      "\n",
      "         [[1.8280]],\n",
      "\n",
      "         [[2.7919]],\n",
      "\n",
      "         [[2.2294]],\n",
      "\n",
      "         [[1.6375]],\n",
      "\n",
      "         [[2.5671]],\n",
      "\n",
      "         [[2.2431]],\n",
      "\n",
      "         [[2.2137]],\n",
      "\n",
      "         [[2.2780]],\n",
      "\n",
      "         [[2.1692]],\n",
      "\n",
      "         [[1.7736]],\n",
      "\n",
      "         [[2.4956]],\n",
      "\n",
      "         [[3.2320]],\n",
      "\n",
      "         [[2.8520]],\n",
      "\n",
      "         [[2.0647]],\n",
      "\n",
      "         [[2.2404]],\n",
      "\n",
      "         [[1.9501]],\n",
      "\n",
      "         [[2.8246]],\n",
      "\n",
      "         [[2.0179]],\n",
      "\n",
      "         [[2.0815]],\n",
      "\n",
      "         [[2.0521]],\n",
      "\n",
      "         [[2.0870]],\n",
      "\n",
      "         [[2.1409]],\n",
      "\n",
      "         [[2.3857]],\n",
      "\n",
      "         [[2.5966]],\n",
      "\n",
      "         [[1.8464]],\n",
      "\n",
      "         [[2.9734]],\n",
      "\n",
      "         [[2.9018]],\n",
      "\n",
      "         [[2.5088]],\n",
      "\n",
      "         [[2.0640]],\n",
      "\n",
      "         [[1.8745]],\n",
      "\n",
      "         [[2.7479]],\n",
      "\n",
      "         [[2.2950]],\n",
      "\n",
      "         [[1.8686]],\n",
      "\n",
      "         [[1.9055]],\n",
      "\n",
      "         [[2.4912]],\n",
      "\n",
      "         [[2.2883]],\n",
      "\n",
      "         [[1.6729]],\n",
      "\n",
      "         [[1.6037]],\n",
      "\n",
      "         [[1.8012]],\n",
      "\n",
      "         [[1.9007]],\n",
      "\n",
      "         [[2.2365]],\n",
      "\n",
      "         [[2.5540]],\n",
      "\n",
      "         [[2.2863]],\n",
      "\n",
      "         [[2.6268]],\n",
      "\n",
      "         [[1.9033]],\n",
      "\n",
      "         [[2.5352]],\n",
      "\n",
      "         [[2.0615]],\n",
      "\n",
      "         [[2.2027]],\n",
      "\n",
      "         [[2.6182]],\n",
      "\n",
      "         [[4.2087]],\n",
      "\n",
      "         [[2.2465]],\n",
      "\n",
      "         [[2.0593]],\n",
      "\n",
      "         [[1.9460]],\n",
      "\n",
      "         [[1.6174]],\n",
      "\n",
      "         [[2.0741]],\n",
      "\n",
      "         [[2.7696]],\n",
      "\n",
      "         [[1.8635]],\n",
      "\n",
      "         [[2.2846]],\n",
      "\n",
      "         [[2.0707]],\n",
      "\n",
      "         [[1.8081]],\n",
      "\n",
      "         [[2.6168]],\n",
      "\n",
      "         [[2.5347]],\n",
      "\n",
      "         [[2.0814]],\n",
      "\n",
      "         [[2.4386]],\n",
      "\n",
      "         [[1.9225]],\n",
      "\n",
      "         [[2.9386]],\n",
      "\n",
      "         [[2.1805]],\n",
      "\n",
      "         [[2.7530]],\n",
      "\n",
      "         [[2.3576]],\n",
      "\n",
      "         [[2.1567]],\n",
      "\n",
      "         [[1.5095]],\n",
      "\n",
      "         [[2.4941]],\n",
      "\n",
      "         [[1.6407]],\n",
      "\n",
      "         [[2.4572]],\n",
      "\n",
      "         [[1.7890]],\n",
      "\n",
      "         [[2.1451]],\n",
      "\n",
      "         [[2.1357]],\n",
      "\n",
      "         [[2.4124]],\n",
      "\n",
      "         [[2.4561]],\n",
      "\n",
      "         [[1.2980]],\n",
      "\n",
      "         [[2.3459]],\n",
      "\n",
      "         [[2.1213]],\n",
      "\n",
      "         [[1.9575]],\n",
      "\n",
      "         [[2.3679]],\n",
      "\n",
      "         [[2.4381]],\n",
      "\n",
      "         [[2.6535]],\n",
      "\n",
      "         [[2.1552]],\n",
      "\n",
      "         [[1.8724]],\n",
      "\n",
      "         [[2.4873]],\n",
      "\n",
      "         [[2.8505]],\n",
      "\n",
      "         [[1.9707]],\n",
      "\n",
      "         [[2.2317]],\n",
      "\n",
      "         [[1.6230]],\n",
      "\n",
      "         [[2.1747]],\n",
      "\n",
      "         [[2.3389]],\n",
      "\n",
      "         [[2.9889]],\n",
      "\n",
      "         [[2.0012]],\n",
      "\n",
      "         [[2.9284]],\n",
      "\n",
      "         [[2.2977]],\n",
      "\n",
      "         [[2.5081]],\n",
      "\n",
      "         [[1.9867]],\n",
      "\n",
      "         [[2.1796]],\n",
      "\n",
      "         [[2.6840]],\n",
      "\n",
      "         [[1.9411]],\n",
      "\n",
      "         [[2.6906]],\n",
      "\n",
      "         [[3.1380]],\n",
      "\n",
      "         [[3.6251]],\n",
      "\n",
      "         [[1.8860]],\n",
      "\n",
      "         [[2.4300]],\n",
      "\n",
      "         [[1.8875]],\n",
      "\n",
      "         [[2.2860]],\n",
      "\n",
      "         [[2.4078]],\n",
      "\n",
      "         [[2.6624]],\n",
      "\n",
      "         [[2.9714]],\n",
      "\n",
      "         [[2.4021]],\n",
      "\n",
      "         [[2.4393]],\n",
      "\n",
      "         [[2.3088]],\n",
      "\n",
      "         [[2.2458]],\n",
      "\n",
      "         [[2.1109]],\n",
      "\n",
      "         [[2.0047]],\n",
      "\n",
      "         [[3.4779]],\n",
      "\n",
      "         [[2.2400]],\n",
      "\n",
      "         [[1.7882]],\n",
      "\n",
      "         [[2.4227]],\n",
      "\n",
      "         [[2.2277]],\n",
      "\n",
      "         [[2.6163]],\n",
      "\n",
      "         [[2.3749]],\n",
      "\n",
      "         [[1.8955]],\n",
      "\n",
      "         [[1.7242]],\n",
      "\n",
      "         [[1.7086]],\n",
      "\n",
      "         [[2.3536]],\n",
      "\n",
      "         [[2.5343]],\n",
      "\n",
      "         [[2.3972]],\n",
      "\n",
      "         [[2.1311]],\n",
      "\n",
      "         [[1.5674]],\n",
      "\n",
      "         [[2.0139]],\n",
      "\n",
      "         [[2.6823]],\n",
      "\n",
      "         [[2.3780]],\n",
      "\n",
      "         [[2.2142]],\n",
      "\n",
      "         [[2.7011]],\n",
      "\n",
      "         [[3.0217]],\n",
      "\n",
      "         [[1.8383]],\n",
      "\n",
      "         [[1.9909]],\n",
      "\n",
      "         [[2.0137]],\n",
      "\n",
      "         [[2.4976]],\n",
      "\n",
      "         [[2.4020]],\n",
      "\n",
      "         [[2.9166]],\n",
      "\n",
      "         [[2.0700]],\n",
      "\n",
      "         [[2.4286]],\n",
      "\n",
      "         [[1.6544]],\n",
      "\n",
      "         [[2.5542]],\n",
      "\n",
      "         [[3.0807]],\n",
      "\n",
      "         [[1.4690]],\n",
      "\n",
      "         [[1.9192]],\n",
      "\n",
      "         [[1.6830]],\n",
      "\n",
      "         [[2.9616]],\n",
      "\n",
      "         [[2.3694]],\n",
      "\n",
      "         [[2.1909]],\n",
      "\n",
      "         [[2.3212]],\n",
      "\n",
      "         [[1.3754]],\n",
      "\n",
      "         [[2.4911]],\n",
      "\n",
      "         [[2.1322]],\n",
      "\n",
      "         [[1.9257]],\n",
      "\n",
      "         [[1.8787]],\n",
      "\n",
      "         [[2.1314]],\n",
      "\n",
      "         [[2.1643]],\n",
      "\n",
      "         [[2.7952]],\n",
      "\n",
      "         [[1.8945]],\n",
      "\n",
      "         [[1.7649]],\n",
      "\n",
      "         [[1.9276]],\n",
      "\n",
      "         [[2.0419]],\n",
      "\n",
      "         [[1.9416]],\n",
      "\n",
      "         [[1.9120]],\n",
      "\n",
      "         [[1.8378]],\n",
      "\n",
      "         [[2.1514]],\n",
      "\n",
      "         [[2.6498]],\n",
      "\n",
      "         [[2.0899]],\n",
      "\n",
      "         [[1.5822]],\n",
      "\n",
      "         [[2.4586]],\n",
      "\n",
      "         [[1.7063]],\n",
      "\n",
      "         [[1.8672]],\n",
      "\n",
      "         [[1.6591]],\n",
      "\n",
      "         [[2.2734]],\n",
      "\n",
      "         [[2.4314]],\n",
      "\n",
      "         [[2.3344]],\n",
      "\n",
      "         [[2.1080]],\n",
      "\n",
      "         [[2.1224]],\n",
      "\n",
      "         [[1.7729]],\n",
      "\n",
      "         [[1.6907]],\n",
      "\n",
      "         [[1.8033]],\n",
      "\n",
      "         [[1.8780]],\n",
      "\n",
      "         [[2.7853]],\n",
      "\n",
      "         [[2.3061]],\n",
      "\n",
      "         [[2.2388]],\n",
      "\n",
      "         [[1.3334]],\n",
      "\n",
      "         [[2.2383]],\n",
      "\n",
      "         [[2.3770]],\n",
      "\n",
      "         [[2.5903]],\n",
      "\n",
      "         [[2.7431]],\n",
      "\n",
      "         [[1.7054]],\n",
      "\n",
      "         [[1.4058]],\n",
      "\n",
      "         [[2.2978]],\n",
      "\n",
      "         [[2.1002]],\n",
      "\n",
      "         [[2.0901]],\n",
      "\n",
      "         [[2.9389]],\n",
      "\n",
      "         [[3.0775]],\n",
      "\n",
      "         [[1.4429]],\n",
      "\n",
      "         [[2.2498]],\n",
      "\n",
      "         [[1.7133]],\n",
      "\n",
      "         [[2.1157]],\n",
      "\n",
      "         [[2.6551]],\n",
      "\n",
      "         [[2.4348]],\n",
      "\n",
      "         [[2.3103]],\n",
      "\n",
      "         [[1.6518]],\n",
      "\n",
      "         [[1.5915]],\n",
      "\n",
      "         [[1.9162]],\n",
      "\n",
      "         [[1.7524]],\n",
      "\n",
      "         [[2.4316]],\n",
      "\n",
      "         [[2.1814]],\n",
      "\n",
      "         [[1.5174]],\n",
      "\n",
      "         [[2.8002]],\n",
      "\n",
      "         [[1.7747]],\n",
      "\n",
      "         [[2.4288]],\n",
      "\n",
      "         [[2.1173]],\n",
      "\n",
      "         [[2.1553]],\n",
      "\n",
      "         [[2.1279]],\n",
      "\n",
      "         [[2.5539]],\n",
      "\n",
      "         [[1.5989]],\n",
      "\n",
      "         [[2.9487]],\n",
      "\n",
      "         [[1.9264]],\n",
      "\n",
      "         [[3.0144]],\n",
      "\n",
      "         [[2.2873]],\n",
      "\n",
      "         [[1.7999]],\n",
      "\n",
      "         [[1.9082]],\n",
      "\n",
      "         [[1.5967]],\n",
      "\n",
      "         [[2.0413]],\n",
      "\n",
      "         [[3.0078]],\n",
      "\n",
      "         [[2.2875]],\n",
      "\n",
      "         [[1.7911]],\n",
      "\n",
      "         [[2.3022]],\n",
      "\n",
      "         [[2.7623]],\n",
      "\n",
      "         [[1.2724]],\n",
      "\n",
      "         [[2.2341]],\n",
      "\n",
      "         [[2.3140]],\n",
      "\n",
      "         [[3.0499]]],\n",
      "\n",
      "\n",
      "        [[[1.7566]],\n",
      "\n",
      "         [[1.4534]],\n",
      "\n",
      "         [[3.1453]],\n",
      "\n",
      "         [[3.0742]],\n",
      "\n",
      "         [[1.6372]],\n",
      "\n",
      "         [[2.3507]],\n",
      "\n",
      "         [[3.1514]],\n",
      "\n",
      "         [[2.3819]],\n",
      "\n",
      "         [[2.0575]],\n",
      "\n",
      "         [[2.3420]],\n",
      "\n",
      "         [[1.8193]],\n",
      "\n",
      "         [[2.6351]],\n",
      "\n",
      "         [[2.3434]],\n",
      "\n",
      "         [[1.7734]],\n",
      "\n",
      "         [[2.0522]],\n",
      "\n",
      "         [[2.2704]],\n",
      "\n",
      "         [[1.5782]],\n",
      "\n",
      "         [[2.1841]],\n",
      "\n",
      "         [[1.8834]],\n",
      "\n",
      "         [[1.8402]],\n",
      "\n",
      "         [[3.5515]],\n",
      "\n",
      "         [[2.4886]],\n",
      "\n",
      "         [[3.9451]],\n",
      "\n",
      "         [[2.1413]],\n",
      "\n",
      "         [[1.8438]],\n",
      "\n",
      "         [[2.2609]],\n",
      "\n",
      "         [[2.2770]],\n",
      "\n",
      "         [[1.9286]],\n",
      "\n",
      "         [[1.6917]],\n",
      "\n",
      "         [[2.7774]],\n",
      "\n",
      "         [[2.3871]],\n",
      "\n",
      "         [[1.7418]],\n",
      "\n",
      "         [[2.1769]],\n",
      "\n",
      "         [[2.3262]],\n",
      "\n",
      "         [[2.1188]],\n",
      "\n",
      "         [[2.2807]],\n",
      "\n",
      "         [[2.8878]],\n",
      "\n",
      "         [[1.5347]],\n",
      "\n",
      "         [[2.4319]],\n",
      "\n",
      "         [[2.3746]],\n",
      "\n",
      "         [[2.8828]],\n",
      "\n",
      "         [[2.0256]],\n",
      "\n",
      "         [[1.5044]],\n",
      "\n",
      "         [[2.8695]],\n",
      "\n",
      "         [[2.2163]],\n",
      "\n",
      "         [[2.0151]],\n",
      "\n",
      "         [[2.9336]],\n",
      "\n",
      "         [[1.5338]],\n",
      "\n",
      "         [[2.7839]],\n",
      "\n",
      "         [[1.8468]],\n",
      "\n",
      "         [[1.4071]],\n",
      "\n",
      "         [[2.8282]],\n",
      "\n",
      "         [[2.3405]],\n",
      "\n",
      "         [[2.0027]],\n",
      "\n",
      "         [[1.9332]],\n",
      "\n",
      "         [[2.6694]],\n",
      "\n",
      "         [[1.6954]],\n",
      "\n",
      "         [[2.2951]],\n",
      "\n",
      "         [[2.0872]],\n",
      "\n",
      "         [[1.9753]],\n",
      "\n",
      "         [[2.3361]],\n",
      "\n",
      "         [[1.6969]],\n",
      "\n",
      "         [[2.3195]],\n",
      "\n",
      "         [[1.8973]],\n",
      "\n",
      "         [[1.5634]],\n",
      "\n",
      "         [[2.1781]],\n",
      "\n",
      "         [[3.4038]],\n",
      "\n",
      "         [[1.8985]],\n",
      "\n",
      "         [[1.9008]],\n",
      "\n",
      "         [[2.2400]],\n",
      "\n",
      "         [[1.6897]],\n",
      "\n",
      "         [[1.5778]],\n",
      "\n",
      "         [[2.0324]],\n",
      "\n",
      "         [[2.6305]],\n",
      "\n",
      "         [[2.3816]],\n",
      "\n",
      "         [[2.1934]],\n",
      "\n",
      "         [[2.4778]],\n",
      "\n",
      "         [[2.0275]],\n",
      "\n",
      "         [[2.3346]],\n",
      "\n",
      "         [[2.4965]],\n",
      "\n",
      "         [[1.7562]],\n",
      "\n",
      "         [[2.4332]],\n",
      "\n",
      "         [[2.0371]],\n",
      "\n",
      "         [[2.0257]],\n",
      "\n",
      "         [[2.2145]],\n",
      "\n",
      "         [[1.7463]],\n",
      "\n",
      "         [[1.9313]],\n",
      "\n",
      "         [[1.9883]],\n",
      "\n",
      "         [[2.2160]],\n",
      "\n",
      "         [[1.5468]],\n",
      "\n",
      "         [[2.5720]],\n",
      "\n",
      "         [[1.5697]],\n",
      "\n",
      "         [[1.6975]],\n",
      "\n",
      "         [[1.9449]],\n",
      "\n",
      "         [[2.4806]],\n",
      "\n",
      "         [[2.5366]],\n",
      "\n",
      "         [[2.9613]],\n",
      "\n",
      "         [[2.2342]],\n",
      "\n",
      "         [[3.4253]],\n",
      "\n",
      "         [[1.6806]],\n",
      "\n",
      "         [[1.8332]],\n",
      "\n",
      "         [[1.7674]],\n",
      "\n",
      "         [[1.8682]],\n",
      "\n",
      "         [[1.3035]],\n",
      "\n",
      "         [[2.0801]],\n",
      "\n",
      "         [[2.2871]],\n",
      "\n",
      "         [[2.3831]],\n",
      "\n",
      "         [[2.2116]],\n",
      "\n",
      "         [[2.3889]],\n",
      "\n",
      "         [[2.4114]],\n",
      "\n",
      "         [[2.2334]],\n",
      "\n",
      "         [[1.8308]],\n",
      "\n",
      "         [[1.4117]],\n",
      "\n",
      "         [[2.1734]],\n",
      "\n",
      "         [[1.7027]],\n",
      "\n",
      "         [[2.6224]],\n",
      "\n",
      "         [[1.6826]],\n",
      "\n",
      "         [[3.0914]],\n",
      "\n",
      "         [[2.1348]],\n",
      "\n",
      "         [[1.9333]],\n",
      "\n",
      "         [[2.3787]],\n",
      "\n",
      "         [[1.7371]],\n",
      "\n",
      "         [[1.9954]],\n",
      "\n",
      "         [[2.3998]],\n",
      "\n",
      "         [[1.4527]],\n",
      "\n",
      "         [[2.0712]],\n",
      "\n",
      "         [[2.1770]],\n",
      "\n",
      "         [[2.5288]],\n",
      "\n",
      "         [[2.2798]],\n",
      "\n",
      "         [[2.0041]],\n",
      "\n",
      "         [[2.9159]],\n",
      "\n",
      "         [[2.1975]],\n",
      "\n",
      "         [[2.2944]],\n",
      "\n",
      "         [[1.5298]],\n",
      "\n",
      "         [[2.8120]],\n",
      "\n",
      "         [[1.7796]],\n",
      "\n",
      "         [[2.2064]],\n",
      "\n",
      "         [[2.6366]],\n",
      "\n",
      "         [[2.3993]],\n",
      "\n",
      "         [[2.3199]],\n",
      "\n",
      "         [[2.8412]],\n",
      "\n",
      "         [[2.1034]],\n",
      "\n",
      "         [[2.0738]],\n",
      "\n",
      "         [[2.0704]],\n",
      "\n",
      "         [[2.1628]],\n",
      "\n",
      "         [[2.2085]],\n",
      "\n",
      "         [[2.3396]],\n",
      "\n",
      "         [[2.6661]],\n",
      "\n",
      "         [[1.8606]],\n",
      "\n",
      "         [[2.2631]],\n",
      "\n",
      "         [[2.7452]],\n",
      "\n",
      "         [[2.0991]],\n",
      "\n",
      "         [[2.5671]],\n",
      "\n",
      "         [[2.8947]],\n",
      "\n",
      "         [[2.7277]],\n",
      "\n",
      "         [[2.3235]],\n",
      "\n",
      "         [[1.9590]],\n",
      "\n",
      "         [[2.7131]],\n",
      "\n",
      "         [[1.9839]],\n",
      "\n",
      "         [[2.3639]],\n",
      "\n",
      "         [[2.4092]],\n",
      "\n",
      "         [[2.4540]],\n",
      "\n",
      "         [[2.2497]],\n",
      "\n",
      "         [[2.3868]],\n",
      "\n",
      "         [[2.5121]],\n",
      "\n",
      "         [[1.5279]],\n",
      "\n",
      "         [[1.7039]],\n",
      "\n",
      "         [[2.2925]],\n",
      "\n",
      "         [[2.2138]],\n",
      "\n",
      "         [[1.7478]],\n",
      "\n",
      "         [[1.4864]],\n",
      "\n",
      "         [[1.9693]],\n",
      "\n",
      "         [[2.4683]],\n",
      "\n",
      "         [[2.0722]],\n",
      "\n",
      "         [[1.8598]],\n",
      "\n",
      "         [[1.9752]],\n",
      "\n",
      "         [[2.2637]],\n",
      "\n",
      "         [[3.0817]],\n",
      "\n",
      "         [[2.2857]],\n",
      "\n",
      "         [[1.6341]],\n",
      "\n",
      "         [[1.6763]],\n",
      "\n",
      "         [[2.4589]],\n",
      "\n",
      "         [[2.0289]],\n",
      "\n",
      "         [[2.0909]],\n",
      "\n",
      "         [[2.7787]],\n",
      "\n",
      "         [[2.2456]],\n",
      "\n",
      "         [[2.6766]],\n",
      "\n",
      "         [[1.5074]],\n",
      "\n",
      "         [[2.7448]],\n",
      "\n",
      "         [[2.2655]],\n",
      "\n",
      "         [[2.0437]],\n",
      "\n",
      "         [[2.2242]],\n",
      "\n",
      "         [[1.9513]],\n",
      "\n",
      "         [[1.6309]],\n",
      "\n",
      "         [[2.1016]],\n",
      "\n",
      "         [[1.7147]],\n",
      "\n",
      "         [[2.2187]],\n",
      "\n",
      "         [[2.8963]],\n",
      "\n",
      "         [[2.6594]],\n",
      "\n",
      "         [[1.8096]],\n",
      "\n",
      "         [[2.3055]],\n",
      "\n",
      "         [[2.0416]],\n",
      "\n",
      "         [[2.2349]],\n",
      "\n",
      "         [[2.1951]],\n",
      "\n",
      "         [[2.0656]],\n",
      "\n",
      "         [[1.4345]],\n",
      "\n",
      "         [[2.3935]],\n",
      "\n",
      "         [[2.1947]],\n",
      "\n",
      "         [[2.3236]],\n",
      "\n",
      "         [[2.5004]],\n",
      "\n",
      "         [[2.2794]],\n",
      "\n",
      "         [[1.5422]],\n",
      "\n",
      "         [[2.3528]],\n",
      "\n",
      "         [[1.8135]],\n",
      "\n",
      "         [[2.3557]],\n",
      "\n",
      "         [[2.1289]],\n",
      "\n",
      "         [[1.8727]],\n",
      "\n",
      "         [[2.2235]],\n",
      "\n",
      "         [[2.0123]],\n",
      "\n",
      "         [[2.0123]],\n",
      "\n",
      "         [[2.4210]],\n",
      "\n",
      "         [[2.3333]],\n",
      "\n",
      "         [[3.1823]],\n",
      "\n",
      "         [[2.9821]],\n",
      "\n",
      "         [[2.7175]],\n",
      "\n",
      "         [[1.8380]],\n",
      "\n",
      "         [[1.4816]],\n",
      "\n",
      "         [[2.1870]],\n",
      "\n",
      "         [[2.5006]],\n",
      "\n",
      "         [[2.3281]],\n",
      "\n",
      "         [[2.2368]],\n",
      "\n",
      "         [[2.8572]],\n",
      "\n",
      "         [[1.3375]],\n",
      "\n",
      "         [[1.8811]],\n",
      "\n",
      "         [[2.5989]],\n",
      "\n",
      "         [[2.0674]],\n",
      "\n",
      "         [[1.9163]],\n",
      "\n",
      "         [[2.7513]],\n",
      "\n",
      "         [[2.3106]],\n",
      "\n",
      "         [[2.2214]],\n",
      "\n",
      "         [[2.1816]],\n",
      "\n",
      "         [[2.1308]],\n",
      "\n",
      "         [[2.4445]],\n",
      "\n",
      "         [[2.4147]],\n",
      "\n",
      "         [[2.2628]],\n",
      "\n",
      "         [[2.2069]],\n",
      "\n",
      "         [[2.4365]],\n",
      "\n",
      "         [[2.1101]],\n",
      "\n",
      "         [[2.7603]],\n",
      "\n",
      "         [[1.6360]],\n",
      "\n",
      "         [[3.0422]],\n",
      "\n",
      "         [[2.0075]],\n",
      "\n",
      "         [[1.9589]],\n",
      "\n",
      "         [[3.1830]],\n",
      "\n",
      "         [[2.1775]],\n",
      "\n",
      "         [[2.9073]],\n",
      "\n",
      "         [[2.3528]],\n",
      "\n",
      "         [[2.1293]],\n",
      "\n",
      "         [[1.8842]],\n",
      "\n",
      "         [[2.0947]],\n",
      "\n",
      "         [[1.7582]],\n",
      "\n",
      "         [[2.2453]],\n",
      "\n",
      "         [[2.6540]],\n",
      "\n",
      "         [[2.7360]],\n",
      "\n",
      "         [[2.4157]],\n",
      "\n",
      "         [[1.9366]],\n",
      "\n",
      "         [[2.2532]],\n",
      "\n",
      "         [[1.7896]],\n",
      "\n",
      "         [[1.5950]],\n",
      "\n",
      "         [[1.9922]],\n",
      "\n",
      "         [[3.2729]],\n",
      "\n",
      "         [[1.8343]],\n",
      "\n",
      "         [[2.0248]],\n",
      "\n",
      "         [[2.1184]],\n",
      "\n",
      "         [[2.3704]],\n",
      "\n",
      "         [[1.9230]],\n",
      "\n",
      "         [[2.2716]],\n",
      "\n",
      "         [[2.1643]],\n",
      "\n",
      "         [[1.8184]],\n",
      "\n",
      "         [[2.3323]],\n",
      "\n",
      "         [[2.3105]],\n",
      "\n",
      "         [[2.0376]],\n",
      "\n",
      "         [[2.7067]],\n",
      "\n",
      "         [[2.2429]],\n",
      "\n",
      "         [[1.8847]],\n",
      "\n",
      "         [[2.7344]],\n",
      "\n",
      "         [[2.4470]],\n",
      "\n",
      "         [[1.9687]],\n",
      "\n",
      "         [[2.3903]],\n",
      "\n",
      "         [[2.2065]],\n",
      "\n",
      "         [[2.1742]],\n",
      "\n",
      "         [[1.9459]],\n",
      "\n",
      "         [[2.2446]],\n",
      "\n",
      "         [[2.1215]],\n",
      "\n",
      "         [[2.1479]],\n",
      "\n",
      "         [[1.9881]],\n",
      "\n",
      "         [[2.0565]],\n",
      "\n",
      "         [[2.2077]],\n",
      "\n",
      "         [[2.6176]],\n",
      "\n",
      "         [[2.0742]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        \n",
    "        # Convolutional layer with kernel size 1x1\n",
    "        self.conv1x1 = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Max pooling with kernel size equal to the feature map size\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply operations sequentially\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_channels = 512\n",
    "output_channels = 300 # D is the desired number of channels\n",
    "transition_layer = TransitionLayer(input_channels, output_channels)\n",
    "\n",
    "# Assuming input_tensor is of size (batch_size, 512, 7, 7)\n",
    "input_tensor = torch.randn(3, 512, 7, 7)\n",
    "\n",
    "# Apply the transition layer\n",
    "output = transition_layer(input_tensor)\n",
    "\n",
    "# Print the shape of the final feature vector\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIIIIG. We know the feature space at the end would be 512, 7, 7. \n",
    "This means that, if we want to create an encoder that would take this; and convert it to a sampling gaussian representation <br>\n",
    "we need to do the following:\n",
    "1. Pass a VGGnet16 pretrained features at eval mode (option to pretrain it too) to 512, 7, 7.\n",
    "2. Pass that VGGnet16 through a transition layer that would flat it to a num_channelsx 300 output (3 since RGB).\n",
    "3. Use this to sample a mean and gaussian distribution uing homework codes to retrieve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that would sample gaussian parameters.\n",
    "Use functions from hw2 utils.py to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def gaussian_parameters(h, dim=-1):\n",
    "    \"\"\"\n",
    "    Converts generic real-valued representations into mean and variance\n",
    "    parameters of a Gaussian distribution\n",
    "\n",
    "    Args:\n",
    "        h: tensor: (batch, ..., dim, ...): Arbitrary tensor\n",
    "        dim: int: (): Dimension along which to split the tensor for mean and\n",
    "            variance\n",
    "\n",
    "    Returns:\n",
    "        m: tensor: (batch, ..., dim / 2, ...): Mean\n",
    "        v: tensor: (batch, ..., dim / 2, ...): Variance\n",
    "    \"\"\"\n",
    "    print(f\"h dimension passed through gaussian paremeters i {h.shape}\")\n",
    "    m, h = torch.split(h, h.size(dim) // 2, dim=dim)\n",
    "    v = F.softplus(h) + 1e-8\n",
    "    return m, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Encoder\n",
    "Now that we have replcated all of the transformations required to perform the encoder first time .<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Projects\\CS236-Final-Proj\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "It can iterate through all batches\n",
      "torch.Size([16, 2])\n",
      "torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,  z_dim, y_dim=0, pretrained=True,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.y_dim = y_dim\n",
    "        # Load pre-trained VGG16 model\n",
    "        vgg16_model = models.vgg16(weights=pretrained)\n",
    "\n",
    "        # Use only the features part and remove the classifier\n",
    "        self.features = vgg16_model.features\n",
    "\n",
    "        # Set to evaluation mode if not fine-tuning\n",
    "        if not pretrained:\n",
    "            self.features.eval()\n",
    "        \n",
    "        # Convolutional layer with kernel size 1x1\n",
    "        self.conv1x1 = nn.Conv2d(512, 300, kernel_size=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm2d(300)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Max pooling with kernel size equal to the feature map size\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=7)\n",
    "\n",
    "        #Obtain the net\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(300, 2 * z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create feature map from vgget16\n",
    "        feat_map = self.features(x)\n",
    "\n",
    "        # Apply operations to obtain transition layer from paper\n",
    "        h = self.conv1x1(feat_map)\n",
    "        h = self.batch_norm(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.max_pool(h)\n",
    "\n",
    "        # Convert output from 3, 300, 1, 1 to 3, 300\n",
    "        h = h.view(h.shape[0], h.shape[1])\n",
    "\n",
    "        #Now pass it through the net to obtain gaussian space\n",
    "        g = self.net(h)\n",
    "\n",
    "        #Pass the feature space and get gaussian parameters\n",
    "        m, v = gaussian_parameters(g, dim=1)\n",
    "        return m, v\n",
    "    \n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#Create encoder compiler\n",
    "encoder_compiler = Encoder(z_dim = 2)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    output = encoder_compiler(images)\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break\n",
    "\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
