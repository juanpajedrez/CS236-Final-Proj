{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Pre trained to test\n",
    "\n",
    "One of the key components for the evaluation of the models is to compare between the VGGnet16 discriminative approach accuracies <br>\n",
    "vs the accuracies obtained by using a VGGnet16 as encoder with a deep generative model in between. <br><br>\n",
    "This jupyter notebook has the objective to, not only retrieve the accuracies of the VGGnet16 pretrained, but to obtain also <br>\n",
    "the layer features before the last classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import necessary modules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "plt.rcParams['figure.figsize'] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to here\n",
    "\n",
    "Make sure the setup the paths properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Python_Projects\\CS236-Final-Proj\\test\n"
     ]
    }
   ],
   "source": [
    "#Path to assign tests (copy path directly)\n",
    "test_path = r\"D:\\Python_Projects\\CS236-Final-Proj\\test\"\n",
    "\n",
    "#Set the path to this working directory\n",
    "os.chdir(test_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "#Append the path the src folder\n",
    "sys.path.append(r'D:\\Python_Projects\\CS236-Final-Proj\\src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary module for downloading\n",
    "\n",
    "Note for this: EVERYTIME There is a change inside the download <br>\n",
    "the changes inside the file would only be shown if the jupyter kernel is restarted. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from utils import CXReader, DfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path\n",
    "data_path = os.path.join(test_path, os.pardir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframes of the data\n",
    "First, lets obtain the dataframes for the data and check that all metadata <br>\n",
    "information has been set up properly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/112124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112124/112124 [00:00<00:00, 498218.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_test.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_train.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_val.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe compiler\n",
    "df_compiler = DfReader()\n",
    "\n",
    "#set the path and retrieve the dataframes\n",
    "df_compiler.set_folder_path(data_path)\n",
    "\n",
    "#Get the dataframe holder and names\n",
    "dfs_holder, dfs_names = df_compiler.get_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the images and labels\n",
    "\n",
    "Also, obtain DataLoaders for test, train, and validation datasets using <br>\n",
    "the Dataloader class from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "It can iterate through all batches\n"
     ]
    }
   ],
   "source": [
    "# Get the device if cuda or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a transformations for the VGGnet16 (requires a 224,224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#Create datasets and dataloaders\n",
    "test_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[0], transform=transform, device=device)\n",
    "train_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[1], transform=transform,device=device)\n",
    "val_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[2], transform=transform, device=device)\n",
    "\n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#With batch size of 32, and shuffle true, and num workers = 4\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vgg16 pretrained model\n",
    "\n",
    "Check if you have GPU Envidia! Else, use the cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Projects\\CS236-Final-Proj\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Python_Projects\\CS236-Final-Proj\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Load the pretrained model\n",
    "vgg16 = models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the last layer\n",
    "We know that VGGnet16 has a last linear layer with 1000 output units...<br>\n",
    "However, this doesnt really resemble our problem per se...<br><br>\n",
    "Lets do this! Lets replace the last layer with a linear layer that has the same <br> number of classes as our data!. (In our case, is 20).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.features)\n",
    "print(vgg16.avgpool)\n",
    "print(vgg16.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=20, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modify the last layer for the last 20 classes\n",
    "num_classes = 20  # Number of classes for your specific task\n",
    "num_features = vgg16.classifier[-1].in_features\n",
    "\n",
    "# Replace the last layer with a new fully connected layer\n",
    "vgg16.classifier[-1] = nn.Linear(num_features, num_classes)\n",
    "vgg16.classifier.add_module(\"sigmoid\", nn.Sigmoid()) #Add sigmoid activation for binary classification between diffs\n",
    "print(vgg16.classifier)\n",
    "\n",
    "# Set requires_grad to False for the modified layer\n",
    "for param in vgg16.classifier[-1].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that would evaluate the model.\n",
    "\n",
    "Make sure it outputs all of the accuracies of all 20 conditions. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, data_loader, limit:int, device:str):\n",
    "    \"\"\"\n",
    "    Instance method that would evaluate with a given\n",
    "    data loader, the accuracies obtained by the VGGNET16\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    #Use no grad to not perform backpropagation for inference time\n",
    "    with torch.no_grad():\n",
    "        #Iterate through each of the images and labels\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "    \n",
    "            #See if it works\n",
    "            images_inputs, images_labels = batch\n",
    "\n",
    "            #Print the shape of each one of them\n",
    "            print(f\"Inputs shape: {images_inputs.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "            #Send the outputs to model in device\n",
    "            outputs = model(images_inputs)\n",
    "\n",
    "            #Binarize the output with threshold\n",
    "            pred_labels = (outputs > threshold).float()\n",
    "\n",
    "            # Calculate TP, FP, TN, FN\n",
    "            TP = torch.sum((pred_labels == 1) & (images_labels == 1)).item()\n",
    "            FP = torch.sum((pred_labels == 1) & (images_labels == 0)).item()\n",
    "            TN = torch.sum((pred_labels == 0) & (images_labels == 0)).item()\n",
    "            FN = torch.sum((pred_labels == 0) & (images_labels == 1)).item()\n",
    "\n",
    "            #_, predicted = torch.max(outputs, 1)  # Get the index of the maximum log-probability\n",
    "            accuracy = ((TP + TN) / (TP + FP + TN + FN)) * 100.0\n",
    "            precision = (TP / (TP + FP)) * 100.0 if (TP + FP) > 0 else 0.0\n",
    "            recall = (TP / (TP + FN)) * 100.0 if (TP + FN) > 0 else 0.0\n",
    "            f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "            print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "            print(\"Precision: {:.2f}%\".format(precision))\n",
    "            print(\"Recall: {:.2f}%\".format(recall))\n",
    "            print(\"F1 Score: {:.2f}%\".format(f1_score))\n",
    "\n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1_score)\n",
    "\n",
    "            if idx == limit:\n",
    "                print(\"Limit reached\")\n",
    "                break\n",
    "    return accuracies, precisions, recalls, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32, 20])\n",
      "Accuracy: 56.25%\n",
      "Precision: 5.97%\n",
      "Recall: 36.36%\n",
      "F1 Score: 10.26%\n",
      "Inputs shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32, 20])\n",
      "Accuracy: 55.62%\n",
      "Precision: 4.17%\n",
      "Recall: 26.19%\n",
      "F1 Score: 7.19%\n",
      "Inputs shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32, 20])\n",
      "Accuracy: 53.12%\n",
      "Precision: 5.19%\n",
      "Recall: 36.59%\n",
      "F1 Score: 9.09%\n",
      "Inputs shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32, 20])\n",
      "Accuracy: 54.22%\n",
      "Precision: 4.98%\n",
      "Recall: 35.00%\n",
      "F1 Score: 8.72%\n",
      "Inputs shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32, 20])\n",
      "Accuracy: 55.62%\n",
      "Precision: 5.86%\n",
      "Recall: 37.21%\n",
      "F1 Score: 10.13%\n",
      "Inputs shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32, 20])\n",
      "Accuracy: 53.75%\n",
      "Precision: 5.26%\n",
      "Recall: 36.59%\n",
      "F1 Score: 9.20%\n",
      "Limit reached\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "accuracies, precisions, recalls, f1_scores= evaluate_model(vgg16, train_loader, 5, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
